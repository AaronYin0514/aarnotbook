---
tags: python ai pytorch 
---

# 矩阵分解

## 常见的矩阵分解

- LU分解：将矩阵A分解成L（下三角）矩阵和U（上三角）矩阵的乘积
- QR分解：将原矩阵分解成一个正交矩阵Q和一个上三角矩阵R的乘积
- EVD分解：特征值分解
- SVD分解：奇异值分解

## EVD分解（特征值分解）

PCA分解

> [特征值分解](https://zh.wikipedia.org/wiki/特征分解)
> **特征分解**（Eigendecomposition），又称**谱分解**（Spectral decomposition）是将矩阵分解为由其**特征值**和**特征向量**表示的矩阵之积的方法。需要注意只有对可对角化矩阵才可以施以特征分解。

> [特征值和特征向量](https://zh.wikipedia.org/wiki/特征分解)
> 
> N维特征向量$\overrightarrow v$是$N \times N$的矩阵$A$的**特征向量**，当且仅当下式成立：
> 
> $A\overrightarrow v = \lambda \overrightarrow v$
> 
> 其中，$\lambda$为一标量，称为$\overrightarrow v$对应的**特征值**。也称$\overrightarrow v$为特征值$\lambda$对应的特征向量。也即特征向量被施以线性变换A只会使向量伸长或缩短而其方向不被改变。

> [矩阵的特征分解](https://zh.wikipedia.org/wiki/特征分解)
> 
> 令$A$是一个$N \times N$的方阵，且有N个线性独立的特征向量$q_i(i = 1,...,N)$。这样$A$被分解为：
> 
> $A = Q \varLambda Q^{-1}$
> 
> 其中$Q$是$N \times N$方阵，且其第$i$列为$A$的特征向量$q_i$。$\varLambda$是对角矩阵，其对角线上的元素为对应的特征值，也即$\varLambda_{ii} = \lambda_i$。
> 
> 只有可对角化矩阵才可以被特征分解。

> [对角矩阵](https://zh.wikipedia.org/wiki/對角矩陣)
> 
> **对角矩阵**（diagonal matrix）是一类除主对角线之外的元素皆为0的矩阵。对角线上的元素可以为0或其他值。
> 
> 因此若n阶方块矩阵$D = (d_{i,j})$符合以下性质：
> 
> $d_{i,j} = 0\ \ if\ \ i \ne j\ \ \ \ \forall i, j \in {1,2,...,n}$
> 
> 例子
> 
> $$
> \begin{bmatrix}
> 1 & 0 & 0 \\
> 0 & 2 & 0 \\
> 0 & 0 & 3
> \end{bmatrix}
> $$

## SVD分解（奇异值分解）

$A = U\varSigma V^T$

