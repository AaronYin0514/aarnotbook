
# 卷积神经网络

卷积神经网：以卷积层为主的深度网络结构

## 卷积层的定义

对图像和滤波矩阵做内积（逐个元素相乘再求和）的操作

```python
nn.Conv2d(in_channels, out_channels, kernel_size, stride = 1, padding = 0, dilation=1, groups=1, bias=True)
```

- kernel_size: 一般采用3 * 3
- stride：移动步长
- padding：边缘填充
- dilation：空洞卷积
- groups：分组卷积
- bias：偏置项

![](juanji.png)

## 常见卷积操作

- 分组卷积（group参数）
- 空洞卷积（dilation参数）
- 深度可分离卷积（分组卷积+ 1 * 1卷积）
- 反卷积（`torch.nn.ConvTranspose2d`，转置卷积）
- 可变形卷积

## 如何理解卷积层感受野

**感受野**（Receptive Field），指的是神经网络中神经元“看到的”输入区域，在卷积神经网络中，feature map上某个元素的计算受输入图像上某个区域的影响，这个区域即该元素的感受野。

![](../assets/imgs/pytorch/ganshouye.png)

## 如何理解卷积层的参数量与计算量

- **参数量**：参与计算参数的个数，占用内存空间
	- $(C_{in} * (K * K) + 1) * C_{out}*$
- **FLOPS**：每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。
- **FLOPs**：浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。
	- $(C_{in} * 2 * K * K) * H_{out} * W_{out} * C_{out}$
- **MAC**：乘加次数，用来衡量计算量。
	- $C_{in} * K * K * H_{out} * W_{out} * C_{out}$

## 如何压缩卷积层参数和计算量

从感受野不变+减少参数数量的角度压缩卷积层

- 采用多个 3 * 3卷积核代替大卷积核
- 采用深度可分离卷积
- 通道Shuffle
- Pooling层
- Stride=2

## 常见的卷积层组合结构

![](../assets/imgs/pytorch/juanjizuhe.png)

### 池化层（Pooling层）

- 对输入的特征图进行压缩
	- 一方面使特征图变小，简化网络计算复杂度
	- 一方面进行特征压缩，提取主要特征

**分类**

- 最大池化（Max Pooling）
- 平均池化（Average Pooling）

**最大池**

```python
nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
```

![](maxpooling.png)

### 上采样层

Resize，如双线性插值直接缩放，类似于图像缩放，概念可见最邻近插值算法和双线性插值算法——图片缩放

Deconvolution，也叫Transposed Convolution

实现函数

```python
nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None)
```

```python
nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, bias=True)
```

### 激活函数层

激活函数：为了增加网络的非线性，进而提升网络的表达能力

ReLU函数，Leakly ReLU函数、ELU函数等

```python
torch.nn.ReLU(inplace=True)
```

### BatchNorm层

通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布

Barchnorm是归一化的一种手段，他会减小图像之间的绝对差异，突出相对差异，加快训练速度

不适用的问题：image-to-image以及对噪声敏感的任务

```python
nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
```

$$
y = \frac \gamma {\sqrt{Var[x] + \varepsilon}} * x + (\beta - \frac {\gamma * E[x]} {\sqrt{Var[x] + \varepsilon}})
$$


### 全连接层（FC层）

链接所有的特征，将输出值送给分类器（如softmax分类器）

- 对前层的特征进行一个加权和，（卷积层是将数据输入映射到隐层特征空间）将特征空间通过线性变换映射到样本标记空间（也就是label）
- 可以通过1 * 1卷积+global average pooling代替
- 可以通过全连接层参数冗余
- 全连接层参数和尺寸相关

```python
nn.Linear(in_features, out_features, bias)
```

### Dropout层

- 在不同的训练过程中随机仍掉一部分神经元
- 测试过程中不使用随机失活，所有的神经元都激活
- 为了防止或减轻过拟合而使用的函数，它一般用在全连接层
- `nn.dropout`


### 损失层

- 损失层：设置一个损失函数用来比较网络输出和目标值，通过最小化损失来驱动网络的训练
- 网络的损失通过前向操作计算，网络参数相对于损失函数的梯度则通过反向操作计算
- 分类问题损失，一般应用于分类和分割问题
	- `nn.BCELoss; nn.CrossEntropyLoss`等
- 回归问题损失，一般应用于检测，回归分类问题
	- `nn.L1loss; nn.MSELoss; nn.SmoothL1Loss;`等


## 经典卷积神经网络结构介绍

| 简单神经网络 | 复杂神经网络      | 轻量型神经网络 |
| ------------ | ----------------- | -------------- |
| LeNet        | ResNet            | MobileNetV1-V3 |
| AlexNet      | InceptionNetV1-V4 | ShuffleNet     |
| VGGNet       | DenseNet          | squeezeNet     |

- 串联结构的经典代表：LeNet、AlexNet、ZFNet、[[050005-VGGNet|VGGNet]]
- 跳连接结构的典型代表：[[050005-VGGNet|ResNet]]
- 并行网络结构典型代表：[[050015-Inception|InceptionV1-V4(GoogLeNet)]]
- 轻量型结构的典型代表：[[050020-MobileNet|MobileNetV1]]
- 多分支结构的典型代表：SiameseNet、TripletNet、QuadrupletNet、多任务网络
- Attention结构典型代表：ResNet + Attention、SENet/Channel + Attention

### Attention机制

- 对于全局信息，注意力机制会重点关注一些特殊的目标区域，也就是所谓的注意力焦点，进而利用有限的注意力资源对信息进行筛选，提高信息处理的准确性和效率
- one-hot分布或者soft的软分布
- Soft-Attention或者Hard-Attention
- 可以作为特征图上，尺度空间上，channel尺度上，不同时刻历史特生上等

## 学习率

- 学习率作为监督学习及深度学习中重要的超参，其决定着目标函数能否收敛到局部最小值以及何时收敛到最小值
- 合适的学习率能够使目标函数在合适的时间内收敛到局部最小值
- `torch.optim.lr_scheduler`
	- ExponentialLR
	- ReduceLROnPlateau
	- CylicLR

## 优化器

- GD、BGD、SGD、MBGD 引入了随机性和噪声
- Momentum、NAG等，加入动量原则，具有加速梯度下降的作用
- AdaGrad，RMSProp，Adam，AdaDelta，自适应学习率

`torch.optim.Adam`

## 卷积神经网络添加正则化

- L1正则：参数绝对值的和
- L2正则：参数的平方和（Pytorch自带，weight_decay）

```python
optimizer = torch.optimSGD(model.parameters(), lr=0.01, weight_decay=0.001)
```





