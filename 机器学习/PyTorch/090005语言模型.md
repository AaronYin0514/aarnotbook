---
tags: python ai pytorch 
---

# 语言模型

从机器学习的角度来看：语言模型是对语句的概率分布的建模

## 语言模型

标准定义：对于语言序列$(w_1,w_2,w_3...)$，语言模型就是计算该序列的概率，即$P(w_1,w_2,w_3...)$

- 统计语言模型（n-gram）  
- 基于前馈神经网络的模型
- 基于循环神经网络的模型

## n-gram

> **马尔可夫假设**（Markov assumption），即假设当前词出现的概率只依赖于前$n-1$个词（共现词频）

给定一个序列的前提下，预测下一个词出现的概率

$n = 1 \text{   unigram:   } P(w_1,w_2,w_3...w_n) = \prod_{i=1}^n{P(w_i)}$

$n = 2 \text{   bigram:   } P(w_1,w_2,w_3...w_n) = \prod_{i=1}^n{P(w_i|w_{i-1})}$

$n = 3 \text{   trigram:   } P(w_1,w_2,w_3...w_n) = \prod_{i=1}^n{P(w_i|w_{i-2} , w_{i-1})}$

## 基于前馈神经网络的模型

Bengio 在03年的这篇经典《A Neural Probabilistic Language Model》中，提出了如下图所示的前馈神经网络结构：

利用神经网络去建模当前词出现的概率与其前 n-1个词之间的约束关系

![](../assets/imgs/pytorch/jiyuqiankui.png)

## 基于循环神经网络的模型

为了解决定长信息的问题，Mikolov 于2010年发表的论文 Recurrentneural network based language model 正式揭开了循环神经网络（RNN）在语言模型中的强大历程RNN

预测当前序列的下一个词

![](../assets/imgs/pytorch/jiyuxunhuan.png)

## 语言模型

- 采用相对熵（relative entropy）来衡量两个分布之间的相近程度
- ﻿﻿**困惑度（perplexity）**

$$
H(W) = -\frac{1}{N}logP(w_1,w_2,w_3...w_N)
$$

$$
\begin{align}

Preplexity(W) & = 2^{H(W)} \\
& = N\sqrt{\frac{1}{P(w_1,w_2,w_3...w_N)}}

\end{align}
$$

## 其他基础概念

- 词向量：将自然语言中的词符号数学化，［0.1.0.2,0.5，..］
- ﻿﻿中文分词
- ﻿﻿词性标注：词汇基本的语法属性
- ﻿﻿句法分析&语法分析（主从关系，主谓宾关系等等）

## NLP主要研究方向和领域

- 语言模型
- ﻿﻿自动分词
- ﻿﻿词法分析旬法分析
- ﻿﻿文本分类，情感分析
- ﻿自动文摘与信息抽取
- ﻿﻿机器翻译，问答系统
- ﻿信息检索，搜索引擎

