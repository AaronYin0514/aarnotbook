---
tags: [tts]
---

# [语音基础知识](https://zhuanlan.zhihu.com/p/510550742)

## 1. 语音基本概念

声波通过空气传播，被麦克风接收，通过**采样、量化、编码**转换为离散的数字信号，即波形文件。**音量、音高和音色是声音的基本属性。**

![](../assets/imgs/tts/000002.jpg)

**1）采样**：原始的语音信号是连续的模拟信号，需要对语音进行采样，转化为时间轴上离散的数据。  
**采样后**，模拟信号被等间隔地取样，这时信号在时间上就不再连续了，但在幅度上还是连续的。经过采样处理之后，模拟信号变成了离散时间信号。  
**采样频率**是指一秒钟内对声音信号的采样次数，采样频率越高声音的还原就越真实越自然。  
在当今的主流采集卡上，采样频率一般共分为22.05KHz、44.1KHz、48KHz三个等级，22.05KHz只能达到FM广播的声音品质，44.1KHz则是理论上的CD音质界限（人耳一般可以感觉到20-20K Hz的声音，根据香农采样定理，采样频率应该不小于最高频率的两倍，所以40KHz是能够将人耳听见的声音进行很好的还原的一个数值，于是CD公司把采样率定为44.1KHz），48KHz则更加精确一些。  
对于高于48KHz的采样频率人耳已无法辨别出来了，所以在电脑上没有多少使用价值。

**2）量化**：进行分级量化，将信号采样的幅度划分成几个区段，把落在某区段的采样到的样品值归成一类，并给出相应的量化值。根据量化间隔是否均匀划分，又分为均匀量化和非均匀量化。  
**均匀量化**的特点为“大信号的信噪比大，小信号的信噪比小”。缺点为“为了保证信噪比要求，编码位数必须足够大，但是这样导致了信道利用率低，如果减少编码位数又不能满足信噪比的要求”（根据信噪比公式，编码位数越大，信噪比越大，通信质量越好）。  
通常对语音信号采用**非均匀量化**，基本方法是对大信号使用大的量化间隔，对小信号使用小的量化间隔。由于小信号时量化间隔变小，其相应的量化噪声功率也减小（根据量化噪声功率公式），从而使小信号时的量化信噪比增大，改善了小信号时的信噪比。  
**量化后**，信号不仅在时间上不再连续，在幅度上也不连续了。经过量化处理之后，离散时间信号变成了数字信号。

**3）编码**：在量化之后信号已经变成了数字信号，需要将数字信号编码成二进制。**“**CD质量**”**的语音采用44100个样本每秒的采样率，每个样本16比特，这个16比特就是编码的位数。

采样，量化，编码的过程称为A/D（从模拟信号到数字信号）转换，如上图1所示。

### 1.1 能量

**音频的能量通常指的是时域上每帧的能量，幅度的平方。**在简单的语音活动检测（Voice Activity Detection，VAD）中，直接利用能量特征：能量大的音频片段是语音，能量小的音频片段是非语音（包括噪音、静音段等）。这种VAD的局限性比较大，正确率也不高，对噪音非常敏感。

```python
def __init__(self, input_file, sr=None, frame_len=512, n_fft=None, win_step=2 / 3, window="hamming"):
        """
        初始化
        :param input_file: 输入音频文件
        :param sr: 所输入音频文件的采样率，默认为None
        :param frame_len: 帧长，默认512个采样点(32ms,16kHz),与窗长相同
        :param n_fft: FFT窗口的长度，默认与窗长相同
        :param win_step: 窗移，默认移动2/3，512*2/3=341个采样点(21ms,16kHz)
        :param window: 窗类型，默认汉明窗
        """
        self.input_file = input_file
        self.frame_len = frame_len  # 帧长，单位采样点数
        self.wave_data, self.sr = librosa.load(self.input_file, sr=sr)
        self.window_len = frame_len  # 窗长512
        if n_fft is None:
            self.fft_num = self.window_len  # 设置NFFT点数与窗长相等
        else:
            self.fft_num = n_fft
        self.win_step = win_step
        self.hop_length = round(self.window_len * win_step)  # 重叠部分采样点数设置为窗长的1/3（1/3~1/2）,即帧移(窗移)2/3
        self.window = window

def energy(self):
        """
        每帧内所有采样点的幅值平方和作为能量值
        :return: 每帧能量值，np.ndarray[shape=(1，n_frames), dtype=float64]
        """
        mag_spec = np.abs(librosa.stft(self.wave_data, n_fft=self.fft_num, hop_length=self.hop_length,
                                       win_length=self.frame_len, window=self.window))
        pow_spec = np.square(mag_spec) # [frequency, time (n_frames)]
        energy = np.sum(pow_spec, axis=0) # [n_frames]
        energy = np.where(energy == 0, np.finfo(np.float64).eps, energy)  # 避免能量值为0，防止后续取log出错(eps是取非负的最小值), 即np.finfo(np.float64).eps = 2.220446049250313e-16
        return energy
```

### 1.2 短时能量

短时能量体现的是信号在不同时刻的强弱程度。设第n帧语音信号的短时能量用$E_n$表示，则其计算公式为：

$$
E_n = \sum_{m=0}^{M-1}\\x_n^2(m)
$$

上式中，M为帧长，  为该帧中的样本点。

```python
def short_time_energy(self):
        """
        计算语音短时能量：每一帧中所有语音信号的平方和
        :return: 语音短时能量列表(值范围0-每帧归一化后能量平方和，这里帧长512，则最大值为512)，
        np.ndarray[shape=(1，无加窗，帧移为0的n_frames), dtype=float64]
        """
        energy = []  # 语音短时能量列表
        energy_sum_per_frame = 0  # 每一帧短时能量累加和
        for i in range(len(self.wave_data)):  # 遍历每一个采样点数据
            energy_sum_per_frame += self.wave_data[i] ** 2  # 求语音信号能量的平方和
            if (i + 1) % self.frame_len == 0:  # 一帧所有采样点遍历结束
                energy.append(energy_sum_per_frame)  # 加入短时能量列表
                energy_sum_per_frame = 0  # 清空和
            elif i == len(self.wave_data) - 1:  # 不满一帧，最后一个采样点
                energy.append(energy_sum_per_frame)  # 将最后一帧短时能量加入列表
        energy = np.array(energy)
        energy = np.where(energy == 0, np.finfo(np.float64).eps, energy)  # 避免能量值为0，防止后续取log出错(eps是取非负的最小值)
        return energy
```

### 1.3 声强和声强级（声压和声压级）

单位时间内通过垂直于声波传播方向的单位面积的平均声能，称作声强，声强用P表示，单位为“瓦/平米”。实验研究表明，人对声音的强弱感觉并不是与声强成正比，而是与其对数成正比，所以一般声强用声强级来表示：

$$
L = 10log(\frac{P}{P'})
$$

其中，$P$为声强，$P' = 10e^{-12}$ 单位 $w/m^2$ 称为基本声强，声强级的常用单位是分贝(dB)。

```python
def intensity(self):
        """
        计算声音强度，用声压级表示：每帧语音在空气中的声压级Sound Pressure Level(SPL)，单位dB
        公式：20*lg(P/Pref)，P为声压（Pa），Pref为参考压力(听力阈值压力)，一般为1.0*10-6 Pa
        这里P认定为声音的幅值：求得每帧所有幅值平方和均值，除以Pref平方，再取10倍lg
        :return: 每帧声压级，dB，np.ndarray[shape=(1，无加窗，帧移为0的n_frames), dtype=float64]
        """
        p0 = 1.0e-6  # 听觉阈限压力auditory threshold pressure: 2.0*10-5 Pa
        e = self.short_time_energy()
        spl = 10 * np.log10(1 / (np.power(p0, 2) * self.frame_len) * e)
        return spl
```

### 1.4 过零率

过零率体现的是信号过零点的次数，体现的是频率特性。

$$
Z_n = \sum^{N-1}_{n=0}\sum^{M-1}_{m=0}|sgn(x_n(m)) - sgn(x_n(m - 1))|
$$

其中，N表示帧数，M表示每一帧中的样本点个数，sgn为符号函数，即：

$$
sgn = 

\begin{cases}

1, & x \ge 0 \\
-1, & x \lt 0

\end{cases}
$$

```python
def zero_crossing_rate(self):
        """
        计算语音短时过零率：单位时间(每帧)穿过横轴（过零）的次数
        :return: 每帧过零率次数列表，np.ndarray[shape=(1，无加窗，帧移为0的n_frames), dtype=uint32]
        """
        zcr = []  # 语音短时过零率列表
        counting_sum_per_frame = 0  # 每一帧过零次数累加和，即过零率
        for i in range(len(self.wave_data)):  # 遍历每一个采样点数据
            if i % self.frame_len == 0:  # 开头采样点无过零，因此每一帧的第一个采样点跳过
                continue
            if self.wave_data[i] * self.wave_data[i - 1] < 0:  # 相邻两个采样点乘积小于0，则说明穿过横轴
                counting_sum_per_frame += 1  # 过零次数加一
            if (i + 1) % self.frame_len == 0:  # 一帧所有采样点遍历结束
                zcr.append(counting_sum_per_frame)  # 加入短时过零率列表
                counting_sum_per_frame = 0  # 清空和
            elif i == len(self.wave_data) - 1:  # 不满一帧，最后一个采样点
                zcr.append(counting_sum_per_frame)  # 将最后一帧短时过零率加入列表
        return np.array(zcr, dtype=np.uint32)
```

### 1.5 基频和基音周期

基音周期反映了声门相邻两次开闭之间的时间间隔，基频（fundamental frequency， F0）则是基音周期的倒数，对应着声带振动的频率，代表声音的音高，声带振动越快，基频越高。如图2所示，蓝色箭头指向的就是基频的位置，决定音高。它是语音激励源的一个重要特征，比如可以通过基频区分性别。一般来说，成年男性基频在 100-250Hz左右，成年女性基频在 150-350Hz左右，女声的音高一般比男声稍高。 人类可感知声音的频率大致在20-20000Hz之间，人类对于基频的感知遵循对数律，也就是说，人们会感觉100Hz到200Hz的差距，与200Hz到400Hz的差距相同。因此，音高常常用基频的对数来表示。

![](../assets/imgs/tts/000003.webp)

### 1.6 音高

音高（pitch）是由声音的基频决定的，音高和基频常常混用。可以这样认为，**音高（pitch）是稀疏离散化的基频（F0）**。由规律振动产生的声音一般都会有基频，比如语音中的元音和浊辅音；也有些声音没有基频，比如人类通过口腔挤压气流的清辅音。在汉语中，元音有a/e/i/o/u，浊辅音有y/w/v，其余音素比如b/p/q/x等均为清辅音，在发音时，可以通过触摸喉咙感受和判断发音所属音素的种类。

```python
def pitch(self, ts_mag=0.25):
        """
        获取每帧音高，即基频，这里应该包括基频和各次谐波，最小的为基频（一次谐波），其他的依次为二次、三次...谐波
        各次谐波等于基频的对应倍数，因此基频也等于各次谐波除以对应的次数，精确些等于所有谐波之和除以谐波次数之和
        :param ts_mag: 幅值倍乘因子阈值，>0，大于np.average(np.nonzero(magnitudes)) * ts_mag则认为对应的音高有效,默认0.25
        :return: 每帧基频及其对应峰的幅值(>0)，
                 np.ndarray[shape=(1 + n_fft/2，n_frames), dtype=float32]，（257，全部采样点数/(512*2/3)+1）
        """
        mag_spec = np.abs(librosa.stft(self.wave_data, n_fft=self.fft_num, hop_length=self.hop_length,
                                       win_length=self.frame_len, window=self.window))
        pitches, magnitudes = librosa.piptrack(S=mag_spec, sr=self.sr, threshold=1.0, ref=np.mean,
                                               fmin=50, fmax=500)  # 人类正常说话基频最大可能范围50-500Hz
        ts = np.average(magnitudes[np.nonzero(magnitudes)]) * ts_mag
        pit_likely = pitches
        mag_likely = magnitudes
        pit_likely[magnitudes < ts] = 0
        mag_likely[magnitudes < ts] = 0
        return pit_likely, mag_likely
pitches, mags = self.pitch()  # 获取每帧基频
f0_likely = []  # 可能的基频F0
for i in range(pitches.shape[1]):  # 按列遍历非0最小值，作为每帧可能的F0
    try:
        f0_likely.append(np.min(pitches[np.nonzero(pitches[:, i]), i]))
    except ValueError:
        f0_likely.append(np.nan)  # 当一列，即一帧全为0时，赋值最小值为nan
f0_all = np.array(f0_likely)
```

### 1.7 共振峰

声门处的准周期激励进入声道时会引起共振特性，产生一组共振频率，这一组共振频率称为共振峰频率或简称共振峰。共振峰包含在语音的频谱包络中，频谱极大值就是共振峰。**频率最低的共振峰称为第一共振峰，对应的频率也称作基频，决定语音的F0，其它的共振峰统称为谐波**，如上图2所示，蓝色箭头指向频谱的第一共振峰，也就是基频的位置，决定音高；而绿框则是其它共振峰，统称为谐波。谐波是基频对应的整数次频率成分，由声带发声带动空气共振形成的，对应着声音三要素的音色。谐波的位置，相邻的距离共同形成了音色特征。谐波之间距离近听起来则偏厚粗，之间距离远听起来偏清澈。在男声变女声的时候，除了基频的移动，还需要调整谐波间的包络，距离等，否则将会丢失音色信息。

### 汇总：

为了有一个直观的图来解释上述的理论，可以把语音波形、短时能量、声强级、过零率、音高绘制在一张图上，如下图3所示：

![](../assets/imgs/tts/000001.png)

## 2. 语音信号的预处理操作

在进行语音特征（如MFCC、频谱图、声谱图等）提取之前一般要进行语音信号的预处理操作，主要包括：预加重、分帧、加窗。

### 2.1 预加重

语音经过说话人的口唇辐射发出，受到唇端辐射抑制，高频能量明显降低。一般来说，当语音信号的频率提高两倍时，其功率谱的幅度下降约6dB，即语音信号的高频部分受到的抑制影响较大。比如像元音等一些因素的发音包含了较多的高频信号的成分，高频信号的丢失，可能会导致音素的共振峰并不明显，使得声学模型对这些音素的建模能力不强。预加重（pre-emphasis）是个一阶高通滤波器，可以提高信号高频部分的能量，给定时域输入信号$x[n]$，预加重之后信号为：

$$
x'[n] = x[n] - a \times x[n - 1]
$$

其中，a是预加重系数，一般取0.97或0.95。如下图4所示，元音音素 /aa/ 原始的频谱图（左）和经过预加重之后的频谱图（右）。

![](../assets/imgs/tts/000004.webp)

```python
def preemphasis(y, coef=0.97, zi=None, return_zf=False):
    """Pre-emphasize an audio signal with a first-order auto-regressive filter:

        y[n] -> y[n] - coef * y[n-1]


    Parameters
    ----------
    y : np.ndarray
        Audio signal

    coef : positive number
        Pre-emphasis coefficient.  Typical values of ``coef`` are between 0 and 1.

        At the limit ``coef=0``, the signal is unchanged.

        At ``coef=1``, the result is the first-order difference of the signal.

        The default (0.97) matches the pre-emphasis filter used in the HTK
        implementation of MFCCs [#]_.

        .. [#] http://htk.eng.cam.ac.uk/

    zi : number
        Initial filter state.  When making successive calls to non-overlapping
        frames, this can be set to the ``zf`` returned from the previous call.
        (See example below.)

        By default ``zi`` is initialized as ``2*y[0] - y[1]``.

    return_zf : boolean
        If ``True``, return the final filter state.
        If ``False``, only return the pre-emphasized signal.

    Returns
    -------
    y_out : np.ndarray
        pre-emphasized signal

    zf : number
        if ``return_zf=True``, the final filter state is also returned
    """

    b = np.asarray([1.0, -coef], dtype=y.dtype)
    a = np.asarray([1.0], dtype=y.dtype)

    if zi is None:
        # Initialize the filter to implement linear extrapolation
        zi = 2 * y[..., 0] - y[..., 1]

    zi = np.atleast_1d(zi)

    y_out, z_f = scipy.signal.lfilter(b, a, y, zi=np.asarray(zi, dtype=y.dtype))

    if return_zf:
        return y_out, z_f

    return y_out


wave_data, self.sr = librosa.load(input_file, sr=sr)  # 音频全部采样点的归一化数组形式数据
wave_data = preemphasis(wave_data, coef=preemph)  # 预加重，系数0.97
```

### 2.2 分帧

语音信号是非平稳信号，考虑到发浊音时声带有规律振动，即基音频率在短时范围内时相对固定的，因此可以认为语音信号具有短时平稳特性，一般认为10ms~50ms的语音信号片段是一个准稳态过程。_短时分析_采用分帧方式，一般每帧帧长为20ms或50ms。假设语音采样率为16kHz，帧长为20ms，则一帧有16000×0.02=320个样本点。

相邻两帧之间的基音有可能发生变化，如两个音节之间，或者声母向韵母过渡。为确保声学特征参数的平滑性，一般采用重叠取帧的方式，即相邻帧之间存在重叠部分。一般来说，帧长和帧移的比例为1:4或1:5。 

> 短时分析：虽然语音信号具有时变特性，但是在一个短时间范围内（一般认为在10-30ms）其特性基本保持相对稳定，即语音具有短时平稳性。所以任何语音信号的分析和处理必须建立在“短时”的基础上，即进行“短时分析”。

```python
def framesig(sig,frame_len,frame_step):
    """Frame a signal into overlapping frames.

    :param sig: the audio signal to frame.
    :param frame_len: length of each frame measured in samples.
    :param frame_step: number of samples after the start of the previous frame that the next frame should begin.
    :returns: an array of frames. Size is NUMFRAMES by frame_len.
    """
    slen = len(sig)
    frame_len = int(round_half_up(frame_len))
    frame_step = int(round_half_up(frame_step))
    if slen <= frame_len:
        numframes = 1
    else:
        numframes = 1 + int(math.ceil((1.0*slen - frame_len)/frame_step))

    padlen = int((numframes-1)*frame_step + frame_len)

    zeros = numpy.zeros((padlen - slen,))
    padsignal = numpy.concatenate((sig,zeros))

    indices = numpy.tile(numpy.arange(0,frame_len),(numframes,1)) + numpy.tile(numpy.arange(0,numframes*frame_step,frame_step),(frame_len,1)).T
    indices = numpy.array(indices,dtype=numpy.int32)
    frames = padsignal[indices]
    return frames

frames = framesig(sig=sig, frame_len=0.030 * sample_rate, # 取帧长为30ms
                                          frame_step=0.006 * sample_rate, # 取帧移为6ms
                                          )
```

### 2.3 加窗

分帧相当于对语音信号加矩形窗（用矩形窗其实就是不加窗），矩形窗在时域上对信号进行截断，在边界处存在多个旁瓣，会发生频谱泄露。为了减少频谱泄露，通常对分帧之后的信号进行其它形式的加窗操作。常用的窗函数有：汉明（Hamming）窗、汉宁（Hanning）窗和布莱克曼（Blackman）窗等。  **加窗主要是为了使时域信号似乎更好地满足FFT处理的周期性要求，减少泄漏（加窗不能消除泄漏，只能减少， 如下图5所示）。**

> 什么是频谱泄露？  
> 音频处理中，经常需要利用傅里叶变换将时域信号转换到频域，而一次快速傅里叶变换（FFT）只能处理有限长的时域信号，但语音信号通常是长的，所以需要将原始语音截断成一帧一帧长度的数据块。这个过程叫信号截断，也叫分帧。分完帧后再对每帧做FFT，得到对应的频域信号。FFT是离散傅里叶变换（DFT）的快速计算方式，而**做DFT有一个先验条件：分帧得到的数据块必须是整数周期的信号，也即是每次截断得到的信号要求是周期主值序列。**但做分帧时，很难满足周期截断，因此就会导致频谱泄露。一句话，频谱泄露就是分析结果中，出现了本来没有的频率分量。比如说，50Hz的纯正弦波，本来只有一种频率分量，分析结果却包含了与50Hz频率相近的其它频率分量。  
非周期的无限长序列，任意截取一段有限长的序列，都不能代表实际信号，分析结果当然与实际信号不一致！也就是会造成频谱泄露。而周期的无限长序列，假设截取的是正好一个或整数个信号周期的序列，这个有限长序列就可以代表原无限长序列，如果分析的方法得当的话，分析结果应该与实际信号一致！因此也就不会造成频谱泄露。

![](../assets/imgs/tts/000005.webp)

汉明窗的窗函数为：$W_{ham}[n] = 0.54 - 0.46cos(\frac{2\pi n}{N} - 1)$  ；汉宁窗的窗函数为：$W_{han}[n] = 0.5[1 - cos(\frac{2\pi n}{N} - 1)]$  ，其中n介于0到N-1之间，N是窗的长度。

加窗就是用一定的窗函数$w(n)$来乘$s(n)$，从而形成加窗语音信号$s_w(n)=s(n)∗w(n)$。

```python
def framesig(sig,frame_len,frame_step,winfunc=lambda x:numpy.ones((x,))):
    """Frame a signal into overlapping frames.

    :param sig: the audio signal to frame.
    :param frame_len: length of each frame measured in samples.
    :param frame_step: number of samples after the start of the previous frame that the next frame should begin.
    :param winfunc: the analysis window to apply to each frame. By default no window is applied.
    :returns: an array of frames. Size is NUMFRAMES by frame_len.
    """
    slen = len(sig)
    frame_len = int(round_half_up(frame_len))
    frame_step = int(round_half_up(frame_step))
    if slen <= frame_len:
        numframes = 1
    else:
        numframes = 1 + int(math.ceil((1.0*slen - frame_len)/frame_step))

    padlen = int((numframes-1)*frame_step + frame_len)

    zeros = numpy.zeros((padlen - slen,))
    padsignal = numpy.concatenate((sig,zeros))

    indices = numpy.tile(numpy.arange(0,frame_len),(numframes,1)) + numpy.tile(numpy.arange(0,numframes*frame_step,frame_step),(frame_len,1)).T
    indices = numpy.array(indices,dtype=numpy.int32)
    frames = padsignal[indices]
    # 加窗操作
    win = numpy.tile(winfunc(frame_len),(numframes,1))
    return frames*win

frames = framesig(sig=sig, frame_len=0.030 * sample_rate, # 取帧长为30ms
                  frame_step=0.006 * sample_rate, # 取帧移为6ms
                  winfunc=np.hamming
                  )
```

## 3. 语音声学特征介绍

[论文笔记：语音情感识别（四）语音特征之声谱图，log梅尔谱，MFCC，deltas](https://www.cnblogs.com/liaohuiqiang/p/10159429.html)

[语音合成基础(3)——关于梅尔频谱你想知道的都在这里](https://zhuanlan.zhihu.com/p/421460202)

上述两篇文章中已经对振幅图、功率谱，log功率谱，mel谱，MFCC等知识有了详细的介绍，故此处不再赘述。