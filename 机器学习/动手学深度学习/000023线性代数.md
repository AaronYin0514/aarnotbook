# 线性代数

## 标量

```python
x = torch.tensor(2.0)
y = torch.tensor(3.0)
  
print(x + y) # tensor(5.)
print(x - y) # tensor(-1.)
print(x * y) # tensor(6.)
print(x / y) # tensor(0.6667)
print(x ** y) # tensor(8.)
```

## 向量

$$
\bf x =
\begin{bmatrix}
x_1,\\
x_2,\\
\vdots \\
x_n
\end{bmatrix}
$$

向量通常记为粗体、小写的符号(例如，$\bf x$、$\bf y$和$\bf z$))

```python
x = torch.arange(4)
print(x) # tensor([0, 1, 2, 3])
```

访问元素

```python
x = torch.arange(4)

e = x[3]
print(e) # tensor(3)
```

长度、形状

```python
x = torch.arange(4)
# 长度
print(len(x)) # 4
# 形状
print(x.shape) # torch.Size([4])
```

## 矩阵

$$
\bf A =
\begin{bmatrix}

a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots& \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn} \\

\end{bmatrix}
$$
矩阵，我们通常用粗体、大写字母来表示(例如，$\bf X$、$\bf Y$和$\bf Z$)

数学表示法使用$\bf A \in \mathbb{R}^{m \times n}$ 来表示矩阵A，其由m行和n列的实值标量组成。

```python
A = torch.arange(20).reshape(5, 4)
print(A)
'''
tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15],
        [16, 17, 18, 19]])
'''
```

矩阵的转置

```python
A = torch.arange(20).reshape(5, 4)
A_T = A.T
print(A_T)

'''
tensor([[ 0,  4,  8, 12, 16],
        [ 1,  5,  9, 13, 17],
        [ 2,  6, 10, 14, 18],
        [ 3,  7, 11, 15, 19]])
'''
```

对称矩阵(symmetric matrix)A等于其转置: $\bf A = \bf A^T$。

```python
A = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])
print(A)
'''
tensor([[1, 2, 3],
        [2, 0, 4],
        [3, 4, 5]])
'''
  
B = A == A.T
print(B)
'''
tensor([[True, True, True],
        [True, True, True],
        [True, True, True]])
'''
```

## 张量

```python
X = torch.arange(24).reshape(2, 3, 4)
print(X)

'''
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
'''
```

> 形状（纬度）是2，3，4
> 
> 从最里面向外看，最里面是最后一维


```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4) 
B = A.clone() # 通过分配新内存，将A的一个副本分配给B 
print(A)
print(A + B)
```

标量乘张量

```python
a = 2
X = torch.arange(24).reshape(2, 3, 4) 
print(a + X)
print((a * X).shape)
```

## 降维

### 降维求和

所有元素求和

```python
x = torch.arange(4, dtype=torch.float32)
print(x.sum()) # tensor(6.)
```

按照某个维度求和-**降维**

```python
X = torch.arange(24).reshape(2, 3, 4)
print(X)
'''
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
'''
  
Y = X.sum(dim=0)
print(Y)
'''
tensor([[12, 14, 16, 18],
        [20, 22, 24, 26],
        [28, 30, 32, 34]])
'''

print(Y.shape) # torch.Size([3, 4])
```

> 解释
> 
> X的形状是(2, 3, 4)，按第0维求和，第0维消失，所以结果的形状是(3, 4)
> 
> 按第0维求和，按前面的方法，从内向外找，第0维就是最外层，它有两个元素，分别是
> 
> ```
> A = [[ 0,  1,  2,  3],
 >     [ 4,  5,  6,  7],
 >     [ 8,  9, 10, 11]]
> 
> B = [[12, 13, 14, 15],
 >     [16, 17, 18, 19],
  >     [20, 21, 22, 23]]
> ```
> 
> 所以A+B就得到：
> 
> ```
> [[12, 14, 16, 18],
>  [20, 22, 24, 26],
 > [28, 30, 32, 34]]
> ```

指定`axis`

```python
X = torch.arange(24).reshape(2, 3, 4)
print(X.sum()) # tensor(276)
print(X.sum(axis=[0,1,2])) # tensor(276)
```

求平均值`mean`

```python
X = torch.arange(24, dtype=torch.float32).reshape(2, 3, 4)
print(X.mean()) # tensor(11.5000)
print(X.sum() / X.numel()) # tensor(11.5000)
```

### 非降维求和

```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
sum_A = A.sum(dim=1, keepdim=True)
print(sum_A)
'''
tensor([[ 6.],
        [22.],
        [38.],
        [54.],
        [70.]])
'''
```

非降维求和应用，可以通过广播将A除以sum_A。

```python
# 求数据占比
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
sum_A = A.sum(dim=1, keepdim=True)
print(A / sum_A)

'''
tensor([[0.0000, 0.1667, 0.3333, 0.5000],
        [0.1818, 0.2273, 0.2727, 0.3182],
        [0.2105, 0.2368, 0.2632, 0.2895],
        [0.2222, 0.2407, 0.2593, 0.2778],
        [0.2286, 0.2429, 0.2571, 0.2714]])
'''
```

沿某个轴计算A元素的累积总和`cumsum`

```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.cumsum(axis=0)
print(B)

'''
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  6.,  8., 10.],
        [12., 15., 18., 21.],
        [24., 28., 32., 36.],
        [40., 45., 50., 55.]])
'''
```

## 点积

```python
x = torch.arange(4, dtype=torch.float32)
y = torch.ones(4, dtype = torch.float32)

z = torch.dot(x, y)
print(z) # tensor(6.)
```

## 矩阵-向量积

```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
x = torch.arange(4, dtype = torch.float32)
z = torch.mv(A, x)
print(z)

'''
tensor([ 14.,  38.,  62.,  86., 110.])
'''
```

## 矩阵-矩阵乘法

```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = torch.ones(4, 3)
z = torch.mm(A, B)
print(z)

'''
tensor([[ 6.,  6.,  6.],
        [22., 22., 22.],
        [38., 38., 38.],
        [54., 54., 54.],
        [70., 70., 70.]])
'''
```

## 范数

### 范数的性质

一、如果我们按常数因子α缩放向量的所有元素，其范数也会按相同常数因子的绝对值缩放:

$$
f(\alpha \rm x) = |\alpha|f(\rm x)
$$

二、三角不等式

$$
f(\rm x + \rm y) \le f(\rm x) + f(\rm y)
$$

三、范数必须是非负的

$$
f(\rm x) \ge 0
$$

四、范数最小为0，当且仅 当向量全由0组成。

$$
\forall i, [x_i] = 0 \Leftrightarrow f(\rm x) = 0
$$
### $L_2$范数

$L_2$范数是向量元素平 方和的平方，在$L_2$范数中常常省略下标2，也就是说$||x||$等同于$||x||_2$ 

$$
||\rm x||_2 = \sqrt{\sum_{i=1}^{n}x_i^2}
$$

```python
u = torch.tensor([3.0, -4.0])
f = torch.norm(u)
print(f)

'''
tensor(5.)
'''
```

### $L_1$范数

$L_1$范数表示为向量元素的绝对值之和

$$
||\rm x||_1 = \sum_{i=1}^n|x_i|
$$
```python
u = torch.tensor([3.0, -4.0])
f = torch.abs(u).sum()
print(f)

'''
tensor(7.)
'''
```

### $L_p$范数

$$
||x||_p = (\sum_{i=1}^n|x_i|^p)^{1/p}
$$

### Frobenius范数

矩阵$X \in \mathbb{Q}^{m \times n}$的Frobenius范数(Frobenius norm)是矩阵元素平方和的平方根:

$$
||\rm X||_F = \sqrt{\sum_{i = 1}^m\sum_{i = 1}^nx_{ij}^2}
$$

```python
f = torch.norm(torch.ones(4, 9))
print(f)

'''
tensor(6.)
'''
```






