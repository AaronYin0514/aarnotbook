# 自动微分

## 一个简单的例子

函数$y = 2x^Tx$关于列向量$x$求导，y是标量。

函数$y = 2x^Tx$关于$x$的梯度应为$4x$

```python
x = torch.arange(4.0)
print('x =', x) # x = tensor([0., 1., 2., 3.])

x.requires_grad_(True) # 等价于x=torch.arange(4.0, requires_grad=True)
print('x.grad =', x.grad) # x.grad = None
  
y = 2 * torch.dot(x, x)
print('y =', y) # y = tensor(28., grad_fn=<MulBackward0>)
  
y.backward()
print('x.grad =', x.grad) # x.grad = tensor([ 0.,  4.,  8., 12.])
  
print(x.grad == 4 * x) # tensor([True, True, True, True])
```

在默认情况下，PyTorch会累积梯度，我们需要清除之前的值`x.grad.zero_()`

```python
y = x.sum()
y.backward()
print(x.grad) # tensor([1., 1., 1., 1.])
```

## 非标量变量的反向传播

对非标量调用`backward`需要传入一个`gradient`参数，该参数指定微分函数关于`self`的梯度。

```python
x = torch.arange(4.0)
y = x * x

y.backward(torch.ones(len(x)))
print(x.grad) # tensor([0., 2., 4., 6.])
```

> 解释：
> 
> $x = [x_0, x_1, x_2, x_3]$
> 
> $y = x * x = [x_0 * x_0, x_1 * x_1, x_2 * x_2, x_3 * x_3]$
> 
> $\frac{\partial y}{\partial x} = ?$ 向量 矩阵 高阶张量没发求导？
> 
> 所以需要传入一个`gradient`参数，用来与y点乘，我们实际传入了$[1, 1, 1, 1]$
> 
> $y = y \dot [1, 1, 1, 1] = x_0^2 + x_1^2 + x_3^3 + x_4^2$
> 
> $\frac{\partial y}{\partial x_1} = 2x_1, \frac{\partial y}{\partial x_2} = 2x_2, \frac{\partial y}{\partial x_3} = 2x_3, \frac{\partial y}{\partial x_4} = 2x_4$
> 
> 所以结果是$[0., 2., 4., 6.]$

`y.sum().backward()`与上面等价，因为求和与点乘[1,1,1,1]等价

```python
x.grad.zero_()
y = x * x
y.sum().backward()
print(x.grad) # tensor([0., 2., 4., 6.])
```

## 分离计算

`torch.Tensor` 是这个包的核心类。如果设置它的属性 `.requires_grad` 为 `True`，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用 `.backward()`，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到`.grad`属性.

要阻止一个张量被跟踪历史，可以调用 `.detach()` 方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。

```python
x = torch.arange(4.0)
y = x * x
u = y.detach()
z = u * x
z.sum().backward()

print(x.grad == u) # tensor([True, True, True, True])
```

## Python控制流的梯度计算

即使构建函数的计算图需要通过Python控制流(例如，条件、循环或任意函数 调用)，我们仍然可以计算得到的变量的梯度。

```python
def f(a):
    b = a * 2
    while b.norm() < 1000:
        b = 2 * b
    if b.sum() > 0:
        c = b
    else:
        c = 100 * b
    return c

a = torch.randn(size=(), requires_grad=True)
d = f(a)
d.backward()

# 这里d的值取决于a的输入，是分段函数，所以用 d/a 判断 
print(a.grad == d / a) # tensor(True)
```


## 参考资料

- [grad can be implicitly created only for scalar outputs](https://blog.csdn.net/qq_39208832/article/details/117415229)

