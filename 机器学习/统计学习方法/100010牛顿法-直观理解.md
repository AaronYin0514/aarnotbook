# 牛顿法-直观理解

## 海森矩阵Hessian metrix

### 定义

海森矩阵是一个自变量为向量的实值两数的二阶偏导数组成的方块矩阵。如果$f(X) = f(x_1,x_2,\dots,x_n)$的所有二阶导数都存在，则$f(X)$的海森知阵为$H$:

$$
H = 

\begin{bmatrix}

\frac{\partial^2f}{\partial x_1^2} & \frac{\partial^2f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2f}{\partial x_1 \partial x_n} \\
\frac{\partial^2f}{\partial x_2 \partial x_1} & \frac{\partial^2f}{\partial x_2^2} & \cdots & \frac{\partial^2f}{\partial x_2 \partial x_n} \\
\vdots& \vdots & \ddots & \vdots \\
\frac{\partial^2f}{\partial x_n \partial x_1} & \frac{\partial^2f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2f}{\partial x_n^2}

\end{bmatrix}
$$


$$
\text{梯度}: 
g(X) =
\begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_n} \\
\end{bmatrix}
$$

$$
H = \frac{dg(X)}{dX} = 

\begin{bmatrix}

\frac{\partial^2f}{\partial x_1^2} & \frac{\partial^2f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2f}{\partial x_1 \partial x_n} \\
\frac{\partial^2f}{\partial x_2 \partial x_1} & \frac{\partial^2f}{\partial x_2^2} & \cdots & \frac{\partial^2f}{\partial x_2 \partial x_n} \\
\vdots& \vdots & \ddots & \vdots \\
\frac{\partial^2f}{\partial x_n \partial x_1} & \frac{\partial^2f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2f}{\partial x_n^2}

\end{bmatrix}
$$

### 海森矩阵性质

1. 对称性：$H^T = H$
2. 如果某一点处的$H$所有特征值都大于0（正定），则$f$在该点附近为严格凸的数。
3. 如果某一点处的$H$的所有特征值都小于0（负定）则$f$在该点附近为严格凹函数。
4. 如果某一点处的$H$的所有特征值有正有负无0，则该点为$f$的局部鞍点。
5. 如果某一点处的$H$的所有特征值有0，则需要进一步判断。
	1. 所有特征值有正有0无负($H$半正定），则$f$在改点附近为不严格凸函数
	2. 所有特征值有负有0无正($H$半货定），则$f$在改点附近为不严格凹函数
	3. 所有特征值有正有0有负(1不定），则不能通过$H$判断$f$在该点附近的凹凸性

![](1111.png)
![](2222.png)
![](3333.png)
![](4444.png)
![](5555.png)

### 多元函数的泰勒展开

如果严格凸两数$f(X) = f(x_1,x_2,\dots,x_n)$在$X_k$（已知）附近的所有阶导数都存在，则$f(X)$在$X_k$处的泰勒展开式为：

$f(X_{k+1}) = f(X_k) + (X_{k+1} - X_k)^Tg_k + \frac{1}{2}(X_{k+1} - X_k)^TH_k(X_{k+1} - X_k) + o||X_{k+1}-X_k||^2$

- $X_{k+1}$ -> 自变量
- $X_k$ -> 常数
- $g_k$ -> 已知

$f(X_{k+1}) \approx f(X_k) + (X_{k+1} - X_k)^Tg_k + \frac{1}{2}(X_{k+1} - X_k)^TH_k(X_{k+1} - X_k)$ 

（二次形）

### 牛顿法

牛顿法思想：用一个空间拋物面逼近$f(X)$在$X_k$附近的两数图像。(二阶逼近）

$\min f(X_{k+1}) \approx \min \{f(X_k) + (X_{k+1} - X_k)^Tg_k + \frac{1}{2}(X_{k+1} - X_k)^TH_k(X_{k+1} - X_k)\}$

> 极小值点如何求？二次形求极小值直接求一阶导

#### 牛顿法迭代公式

$$
\begin{align}
& \frac{d\{f(X_{k})+(X_{k+1}-X_k)^Tg_k + \frac{1}{2}(X_{k+1}-X_k)^TH_k(X_{k+1}-X_k)\}}{dX_{k+1}} \\
& = g_k + \frac{1}{2}[H_k(X_{k+1}-X_k) + H_k^T(X_{k+1}-X_k)] \\
& = g_k + H_k(X_{k+1}-X_k) = 0
\end{align}
$$

$X_{k+1} = X_{k} - H_k^{-1}g_k$ 牛顿迭代公式

#### 牛顿法终止条件

$X_{k+1} = X_{k} - H_k^{-1}g_k$

- 终止条件1（常用）：  $||P_{k+1} - P_k|| \le \xi$
- 终止条件2（不常用）：  $||g_{k+1} - g_k|| \le \xi$

#### 优缺点

**优点**

牛顿法是超线性收敛的，收敛速度快于固定学习率的梯度下降法。

**缺点**

对两数$f(X)$要求较高。既要求$f(X)$为$\color{red}{严格凸两数}$，还要求任意点$X_k$处的$\color{red}{海森矩阵存在且正定}$。

#### 一个简单案例（帮助理解牛顿法过程）

![](niudun.png)

1. 随机选择一个$X_0$点
2. 利用牛顿法在$f(X_0)$处求得抛物线，计算抛物线最小值，然后得到点$X_1$
3. 重复上一步操作，得到点$X_2$
4. 直到求出满意解，例如$X_3$

## 案例与代码

### 案例

例：求函数$f(x_1, x_2) = (x_1 - 1)^4 + 2x_2^2$的最小值

> 简单的数学题，明显答案是$(-1, 2)$，怎样通过牛顿法求解？

**解：** 

$$
\text{梯度}g =

\begin{bmatrix}
\frac{df}{dx_1} \\
\frac{df}{dx_2} \\
\end{bmatrix}

=

\begin{bmatrix}
4(x_1 - 1)^3 \\
4x_2 \\
\end{bmatrix}
$$

$$
\text{海森矩阵}H =

\begin{bmatrix}
\frac{\partial^2f}{\partial x_1^2} & \frac{\partial^2f}{\partial x_1 \partial x_2} \\
\frac{\partial^2f}{\partial x_2 \partial x_1} & \frac{\partial^2f}{\partial x_2^2} \\
\end{bmatrix}

=

\begin{bmatrix}
12(x_1 - 1)^2 & 0 \\
0 & 4 \\
\end{bmatrix}
$$

**函数图像**

```python
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import pyplot as plt

fig = plt.figure()
ax = Axes3D(fig, auto_add_to_figure=False)
fig.add_axes(ax)

x = np.arange(-1, 3, 0.1)
y = np.arange(-2, 3, 0.1)
X, Y = np.meshgrid(x, y)
Z = (X - 1) ** 4 + 2 * Y ** 2

ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow')
plt.xlabel('x')
plt.ylabel('y')

plt.show()
```

![](niudun1111.png)

**等腰线**

```python
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import pyplot as plt

fig = plt.figure()

x = np.arange(-1, 3, 0.1)
y = np.arange(-2, 3, 0.1)
X, Y = np.meshgrid(x, y)
Z = (X - 1) ** 4 + 2 * Y ** 2

plt.xlabel('x')
plt.ylabel('y')

c = plt.contour(X, Y, Z)
plt.clabel(c, inline=True, fontsize=10)
plt.show()
```

![](niudun2222.png)

**牛顿法求解**

```python
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import pyplot as plt

fig = plt.figure()

x = np.arange(-1, 3, 0.1)
y = np.arange(-2, 3, 0.1)
X, Y = np.meshgrid(x, y)
Z = (X - 1) ** 4 + 2 * Y ** 2

plt.xlabel('x')
plt.ylabel('y')

c = plt.contour(X, Y, Z)
plt.clabel(c, inline=True, fontsize=10)

X = np.mat([[-1], [2]])
g = np.ones(shape=(2,1))
heng, zong = [X[0,0]], [X[1,0]]
while np.linalg.norm(g) > 0.001:
  g = np.array([[4 * (X[0,0] - 1) ** 3], [4 * X[1,0]]])
  H = np.array([[12 * (X[0,0]-1)**2, 0.0], [0.0, 4.0]])
  X = X - np.dot(np.linalg.inv(H), g)
  heng.append(X[0, 0])
  zong.append(X[1, 0])

plt.plot(heng, zong, 'b*-')
plt.show()
```

![](niudun3333.png)

**定长梯度下降法**

```python
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import pyplot as plt

fig = plt.figure()

x = np.arange(-1, 3, 0.1)
y = np.arange(-2, 3, 0.1)
X, Y = np.meshgrid(x, y)
Z = (X - 1) ** 4 + 2 * Y ** 2

plt.xlabel('x')
plt.ylabel('y')

c = plt.contour(X, Y, Z)
plt.clabel(c, inline=True, fontsize=10)

'''
定长梯度下降法
'''
X = np.mat([[-1], [2]])
g = np.ones(shape=(2,1))
heng, zong = [X[0,0]], [X[1,0]]
while np.linalg.norm(g) > 0.001:
  g = np.array([[4 * (X[0,0] - 1) ** 3], [4 * X[1,0]]])
  X = X - 0.01 * g
  heng.append(X[0, 0])
  zong.append(X[1, 0])
plt.plot(heng, zong, 'r*-')
plt.show()
```

![](niudun444.png)

红色为**定长梯度下降法**，蓝色为**牛顿法**，可以看出牛顿法迭代次数优于定长梯度下降法

## 参考资料

- [无约束最优化方法之牛顿法](https://www.bilibili.com/video/BV1to4y1x7mj/?spm_id_from=333.999.0.0&vd_source=23d5ee46aa9f762104f00e96b4a39245)

