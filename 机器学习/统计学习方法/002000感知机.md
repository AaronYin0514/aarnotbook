# 感知机

**感知机** (perceptron) 是二类分类的线性分类模型，其输入为实例的特征向量，输 出为实例的类别，取 +1 和 -1 二值。

## 感知机模型

假设输入空间(特征空间)是$\mathcal X \in R^n$， 输出空间是$\mathcal Y = \{+1, -1\}$。输入$x \in \mathcal X$ 表示实例的特征向量，对应于输入空间(特征空间)的点; 输出$y \in \mathcal Y$表示实例的类别。由输入空间到输出空间的如下函数:

$$
f(x) = sign(\omega \cdot x + b)
$$

称为感知机。 其中，$\omega$和$b$为感知机模型参数

- $\omega \in R^n$叫作**权值** (weight) 或**权值向量**(weightvector)
- $b \in R$叫作**偏置**(bias)， 
- $\omega \cdot x$表示$\omega$和$x$的[[000045数学基础-向量#向量的点乘(点积) | 内积]]
- $sign$是**符号函数**，即

$$
sign(x) = 
\begin{cases}
+1, & x \ge 0 \\
-1, & x \lt 0
\end{cases}
$$

## 感知机学习策略

### 数据集的线性可分性

给定一个数据集

$$
T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}
$$

其中，$x_i \in \mathcal X = R^n，y_i \in \mathcal Y = \{+1, -1\}，i=1,2,\dots,N$， 如果存在某个超平面S能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，即对所有$y_i=+1$的实例$i$，有$\omega \cdot x > 0$，对所有$y_i = -1$的实例$i$，有$\omega \cdot x < 0$，则 称数据集 T 为**线性可分数据集**( linearly separable data set ) ;否则，称数据集 T 线性不可分。

### 感知机学习策略

给定训练数据集

$$
T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}
$$

其中，$x_i \in \mathcal X = R^n，y_i \in \mathcal Y = \{+1, -1\}，i=1,2,\dots,N$，感知机$sign(\omega \cdot x + b)$学习的损失函数定义为

$$
L(\omega, b) = -\sum_{x_i \in M}y_i(\omega \cdot x_i + b)
$$

其中 M 为误分类点的集合。这个损失函数就是感知机学习的经验风险函数。

## 感知机学习算法

### 原始形式

**输入**: 训练数据集$T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}$，其中，$x_i \in \mathcal X = R^n，y_i \in \mathcal Y = \{+1, -1\}，i=1,2,\dots,N$，学习率$\eta(0 < \eta \le 1)$

**输出**: $\omega,b$；感知机模型$f(x) = sign(\omega \cdot x + b)$

- （1）选取初$\omega_0,b_0$
- （2）在训练集中选取数据$(x_i,y_i)$
- （3）如果$y_i(\omega \cdot x_i + b) \le 0$，属于**误分类**，更新参数

$$
\omega \gets \omega + \eta y_i x_i
$$

$$
b \gets b + \eta y_i
$$

- （4）转至（2），直至训练集中没有误分类点

### 对偶形式

**输入**: 训练数据集$T=\{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}$，其中，$x_i \in \mathcal X = R^n，y_i \in \mathcal Y = \{+1, -1\}，i=1,2,\dots,N$，学习率$\eta(0 < \eta \le 1)$

**输出**:  $\alpha,b$；感知机模型$f(x) = sign(\sum_{j=1}^N{\alpha_i y_i x_i \cdot x} + b)$，其中$\alpha = (\alpha_1,\alpha_2,\dots,\alpha_N)^T$

- （1）$\alpha \gets 0$，$b \gets 0$
- （2）在训练集中选取数据$(x_i,y_i)$
- （3）如果$y_i(\sum_{j=1}^N{\alpha_i y_i x_i \cdot x} + b) \le 0$，属于**误分类**，更新参数

$$
\alpha_i \gets \alpha_i + \eta
$$

$$
b \gets b + \eta y_i
$$

> 注意⚠️
> 第（2）步我们取$(x_i,y_i)$时，$i$已经确定，那么$\alpha_i$也是确定的。
> 
> 例如取$(x_1,y_1)$，那么如果是误分类点，需要更新$\alpha_1$和$b$，而$\alpha_2 \dots$都不改变。



- （4）转至（2），直到没有误分类数据

例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的 Gram 矩阵

$$
G = [x_i x_j]_{N \times N}
$$









