# 第4章 朴素贝叶斯法

## 4.0 数学基础

- [[000010数学基础-概率基础#条件概率|条件概率&乘法公式&全概率公式&贝叶斯公式]]
- [[000011数学基础-先验概率&后验概率|先验概率&后验概率]]
- [[000040数学基础-极大似然估计与贝叶斯估计|极大似然估计与贝叶斯估计]]

## 4.1 朴素贝叶斯法的学习与分类

### 4.1.1 基本方法

#### 问题

设输入空间$\mathcal X \in \bf{R}^n$为 n 维向量的集合，输出空间为类标记集合$\mathcal Y = \{c_1,c_2,\dots,c_K\}$。输入为特征向量$x \in \mathcal X$， 输出为类标记 (class labe) $y \in \mathcal Y$。$X$是定义在输入空间$\mathcal X$上的随机向量 ，$Y$是定义在输出空间$\mathcal Y$上的随机变量。$P(X,Y)$是 $X$ 和 $Y$ 的联合概率分布。训练数据集

$$
T = \{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}
$$

由 $P(X, Y)$ 独立同分布产生。

朴素贝叶斯法通过训练数据集学习联合概率分布 $P(X,Y)$。

#### 朴素贝叶斯分类器

> **朴素贝叶斯分类器**
> 
> $$
> y = arg \max_{c_k} P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)
> $$

**先验概率分布**

$$
P(Y=c_k), k=1,2,\dots,K
$$

**条件概率分布**

$$
P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},\dots,X^{(n)}=x^{(n)}|Y=c_k),k=1,2,\dots,K
$$

**联合概率分布**

$$
P(X,Y)
$$

条件概率分布 $P(X = x|Y = c_k)$ 有指数级数量的参数，参数个数$K\prod_{j=1}^nS_j$ 。其估计实际是不可行的 。

**朴素贝叶斯法对条件概率分布作了条件独立性的假设。** 这一假设牺牲一定的分类准确率。

$$
P(X=x|Y=c_k) = P(X^{(1)}=x^{(1)},\dots,X^{(n)}=x^{(n)}|Y=c_k) = \prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)
$$

**朴素贝叶斯基本公式推理**

后验概率计算根据贝叶斯定理进行:

$$
P(Y=c_k|X=x) = \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_k P(X=x|Y=c_k)P(Y=c_k)}
$$

带入得**朴素贝叶斯法分类的基本公式**：

$$
P(Y=c_k|X=x) = \frac{P(Y=c_k)\prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_k P(Y=c_k)\prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)},k=1,2,\dots,K
$$

朴素贝叶斯分类器可表示为

$$
y = f(x) = arg\max_{c_k}\frac{P(Y=c_k)\prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_k P(Y=c_k)\prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)}
$$

因为分母对所有 $c_k$ 都是相同的，所以，

$$
y = arg \max_{c_k} P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)
$$

### 4. 1.2 后验概率最大化的含义

朴素贝叶斯法将实例分到后验概率最大的类中。这等价于期望风险最小化。假设 选择。-1 损失函数:

$$
L(Y,f(X)) = 

\begin{cases}
1, & Y \ne f(X) \\
0, & Y = f(X)
\end{cases}
$$

式中 $f(X)$ 是分类决策函数。这时，期望风险函数为

$$
R_{exp}(f) = E[L(Y,f(X))]
$$

期望是对联合分布 $P(X，Y)$ 取的。由此取条件期望

$$
R_{exp}(f) = Ex\sum_{k=1}^K[L(c_k,f(X))]P(c_k|X)
$$

为了使期望风险最小化，只需对 X=x 逐个极小化，由此得到:

$$
\begin{align}
f(x) & = arg \min_{y \in \mathcal Y}\sum_{k=1}^KL(c_k,y)P(c_k|X=x) \\
& = arg \min_{y \in \mathcal Y}\sum_{k=1}^KP(y \ne c_k|X=x) & \text{注释：L函数相等是0} \\
& = arg \min_{y \in \mathcal Y}(1 - P(y = c_k|X=x)) \\
& = arg \max_{y \in \mathcal Y}P(y = c_k|X=x)
& 
\end{align}
$$

根据期望风险最小化准则就得到了后验概率最大化准则 :

$$
f(x) = arg \max_{c_k}P(c_k|X=x)
$$

即朴素贝叶斯法所采用的原理。

## 4.2 朴素贝叶斯法的参数估计

### 4.2.1 极大似然估计

先验概率 $P(Y = c_k)$ 的极大似然估计是

$$
P(Y = c_k) = \frac{\sum_{i=1}^NI(y_i=c_k)}{N}, k=1,2,\dots,K
$$

设第$j$个特征$x^{(j)}$可能取值的集合为$\{a_{j1},a_{j2},\dots,a_{jS_j}\}$，条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计是

$$
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}
$$

$$
j = 1,2,\dots,n; l = 1,2,\dots,S_j; k = 1,2,\dots,K
$$

式中， $x_i^{(j)}$是第$i$个样本的第$j$个特征；$a_{jl}$是第$j$个特征可能取的第$l$个值;

$I$为**指示函数**。

> **推导过程**
> 
> *推导第一式*
> 
> 设$P = P(Y = c_k)$，根据似然估计
> 
> $L = P^{\sum_{i=1}^NI(y_i=c_k)} \cdot (1 - P)^{\sum_{i=1}^NI(y_i \ne c_k)}$
> 
> 两边求自然对数
> 
> $\ln L = \ln(P) \cdot \sum_{i=1}^NI(y_i=c_k) + \ln(1-P) \cdot \sum_{i=1}^NI(y_i \ne c_k)$
> 
> 求导
> 
> $$
> \begin{align}
> \frac{∂\ln L}{∂x} & = \frac{\sum_{i=1}^NI(y_i=c_k)}{P} - \frac{\sum_{i=1}^NI(y_i \ne c_k)}{1-P} \\
> & = \frac{(1-P)\sum_{i=1}^NI(y_i=c_k) - P\sum_{i=1}^NI(y_i \ne c_k)}{P(1-P)} \\
> & = \frac{\sum_{i=1}^NI(y_i=c_k) - P(\sum_{i=1}^NI(y_i=c_k) + \sum_{i=1}^NI(y_i \ne c_k))}{P(1-P)} \\
> & = \frac{\sum_{i=1}^NI(y_i=c_k) - P \cdot N}{P(1-P)} \\
> & = 0 \text{求最大}
> \end{align}
> $$
> 
> 则
> 
> $P = P\{Y=c_k\} = \frac{\sum_i^NI(y_i = c_k)}{N}$
> 
> $I$为指示函数
> 
> *推导第二式*
> 
> 同理
> 
> $P(X^{(j)}=a_{jl},Y=c_k) = \frac{\sum_{j=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{N}$
> 
> 由条件概率知
> 
> $$
> \begin{align}
> P(X^{(j)}=a_{jl},Y=c_k) & = \frac{P(X^{(j)}=a_{jl}Y=c_k)}{P(Y=c_k)} \\
> & = \frac{\frac{\sum_{i=1}^NI(x^{(j)}=a_{jl},y_i=c_k))}{N}}{\frac{\sum_{i=1}^NI(y_i=c_k)}{N}} \\
> & = \frac{\sum_{i=1}^NI(x^{(j)}=a_{jl},y_i=c_k))}{\sum_{i=1}^NI(y_i=c_k)}
> \end{align}
> $$

### 4.2.2 学习与分类算法

**算法 4.1 (朴素贝叶斯算法 (naïve Bayes algorithm) )**

输入:训练数据$T = \{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}$，其中$x_i = (x_i^{(1)},x_i^{(2)},\dots,x_i^{(n)})^T$是第$i$个样本的第$j$个特征，$x_i^{(j)} \in \{a_{j1},a_{j2},\dots,a_{jS_j}\}$，$a_{jl}$是第$j$个特征可能取的第$l$个值，$j=1,2,\dots,n,l=1,2,\dots,S_j,y_i \in \{c_1,c_2,\dots,c_K\}$；实例$x$；

输出: 实例$x$的分类。

- （1）计算**先验概率**及**条件概率**

$$
P(Y=c_k) = \frac{\sum_{i=1}^NI(y_i=c_k)}{N}, k=1,2,\dots,K
$$

$$
P(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}
$$

$$
j = 1,2,\dots,n; l = 1,2,\dots,S_j; k = 1,2,\dots,K
$$

- （2）对于给定的实例$x=(x_i^{(1)},x_i^{(2)},\dots,x_i^{(n)})^T$，计算

$$
P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k),k=1,2,\dots,K
$$

- （3）确定实例$x$的类

$$
y = arg \max_{c_k} P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}|Y=c_k)
$$

### 4.2.3 贝叶斯估计

条件概率的贝叶斯估计是

$$
P_{\lambda}(X^{(j)}=a_{jl}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jl},y_i=c_k) + \lambda}{\sum_{i=1}^NI(y_i=c_k) + S_j\lambda}
$$

式中$\lambda \ge 0$。常取$\lambda = 1$，这时称为拉普拉斯平滑（Laplacian smoothing）。

先验概率的贝叶斯估计是：

$$
P_{\lambda}(Y = c_k) = \frac{\sum_{i=1}^NI(y_i=c_k) + \lambda}{N + K\lambda}
$$



















