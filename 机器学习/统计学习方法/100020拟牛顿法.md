# 拟牛顿法

> [拟牛顿法比较好的文章](https://www.cnblogs.com/ljy2013/p/5129294.html)

**拟牛顿法** (Quasi-Newton Methods) 是求解**非线性优化问题**最有效的方法之一，于20世纪50年代由美国 Argonne 国家实验室的物理学家 W. C. Davidon 提出． Davidon 设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一． 不久 R. Fletcher 和 M.JD. Powell 证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进．在之后的 20 年里，拟牛顿方法得到了蓬物发展，出现了大量的变形公式以及数以百计的相关论文([9]).

DFP 方法、BFGS 方法以及 L-BFGS 算法都是重要的拟牛顿法，本文将对这些方法进行简要介绍．当然，在介绍拟牛顿法之前，我们先看看什么是牛顿法，以及拟牛顿法和牛顿法之问有什么关系． 为此，考虑如下**无约束的极小化问题**

$$
\min\limits_{x} f(x)
$$

其中$x=(x_1,x_2,\dots,x_N)^T\in \mathbb R^N$，由于本文不淮备对收敛性理论进行讨论，因此不妨对**目标函数**$f: \mathbb R^N \to \mathbb R$作一个较苛刻的假设，这里我们假定$f$为**凸函数**，且**两阶连续可微**．此外，记极小问题$(0.1)$的解为$x^*$

## §1 牛顿法

### §1.1 原始牛顿法

为简单起见，首先考虑$N=1$的简单情形(此时目标函数$f(x)$变为$f(x)$)．**牛顿法的基本思想**是：**在现有极小点估计值的附近对$f(x)$做二阶泰勒展开，进而找到极小点的下一个估计值**．设$x_k$为当前的极小点估计值，则

$$
\varphi(x) = f(x_k) + f'(x_k)(x-x_k) + \frac{1}{2}f''(x_k)(x-x_k)^2 \tag{1.2}
$$

表示$f(x)$在$x_k$附近的二阶泰勒展开式（略去了关于$x-x_k$的高阶项）．由于求的是最值，由极值必要条件可知，$\varphi(x)$应满足

$$
\varphi'(x) = 0 \tag{1.3}
$$

即

$$
f'(x_k) + f''(x_k)(x-x_k) = 0 \tag{1.4}
$$

从而求得

$$
x = x_k - \frac{f'(x_k)}{f''(x_k)} \tag{1.5}
$$

于是，若给定初始值$x_0$，则可以构造如下的迭代格式

$$
x_{k+1} = x_k - \frac{f'(x_k)}{f''(x_k)}, k = 0,1,\dots \tag{1.6}
$$

产生序列$\{x_k\}$来逼近$f(x)$的极小点. 在一定条件下，$\{x_k\}$可以收敛到$f(x)$的极小点．

对于$N > 1$的情形，二阶泰勒展开式(1.2)可以做推广，此时

$$
\varphi(x) = f(x_k) + \nabla f(x_k) \cdot (x-x_k) + \frac{1}{2} \cdot (x-x_k)^T \cdot \nabla^2f(x_k) \cdot (x-x_k) \tag{1.7}
$$

其中$\nabla f$为$f$的**梯度向量**，$\nabla^2 f$为$f$的海森矩阵 (Hessian matrix)，其定义分别为

$$
\nabla f = 
\begin{bmatrix}
\frac{\partial f}{\partial x_1} \\
\frac{\partial f}{\partial x_2} \\
\vdots \\
\frac{\partial f}{\partial x_N} \\
\end{bmatrix}

,
\nabla^2f = 
\begin{bmatrix}

\frac{\partial^2f}{\partial x_1^2} & \frac{\partial^2f}{\partial x_1 \partial x_2} & \cdots & \frac{\partial^2f}{\partial x_1 \partial x_N} \\
\frac{\partial^2f}{\partial x_2 \partial x_1} & \frac{\partial^2f}{\partial x_2^2} & \cdots & \frac{\partial^2f}{\partial x_2 \partial x_N} \\
\vdots& \vdots & \ddots & \vdots \\
\frac{\partial^2f}{\partial x_N \partial x_1} & \frac{\partial^2f}{\partial x_N \partial x_2} & \cdots & \frac{\partial^2f}{\partial x_N^2}

\end{bmatrix}

\tag{1.8}
$$

注意，$\nabla f$和$\nabla^2 f$中的元素均为关于$x$的函数，以下分别将其简记为$g$和$H$. 特别地，若$f$的混合偏导数可交换次序(即对$\forall i,j$，成立$\frac{\partial^2f}{\partial x_i \partial x_j} = \frac{\partial^2f}{\partial x_j \partial x_i}$)，则海森矩阵$H$为**对称矩阵**。而$\nabla f(x_k)$和$\nabla^2 f(x_k)$则表示将$x$取为$x_k$后得到的实值向量和矩阵，以下分别将其简记为$g_k$和$H_k$（这里字母g表示gradient，H表示Hessian）。

同样地，由于是求极小点，极值必要条件要求它为$\varphi(x)$的**驻点**，即

$$
\nabla \varphi(x) = 0 \tag{1.9}
$$

亦即（通过在(1.7）两边作用一个梯度算子）

$$
g_k + H_k \cdot (x - x_k) = 0 \tag{1.10}
$$

进一步，若矩阵$H_k$非奇异，则可解得

$$
x = x_k - H_K^{-1} \cdot g_k \tag{1.11}
$$

手是，若给定初始值$x_0$，则同样可以构造出迭代格式

$$
x_{k+1} = x_k - H_k^{-1} \cdot g_k, k = 0,1,\dots \tag{1.12}
$$

这就是**原始的牛顿迭代法**，其选代格式中的搜索方向$d_k = -H_k^{-1} \cdot g_k$称为**牛顿方向**．下面给出一个牛顿法的完整算法描述.

> **算法1.1**（牛顿法）
> 
> 1. 给定初值$x_0$和精度阀值$\epsilon$ ，并令$k := 0$
> 2. 计算$g_k$和$H_k$
> 3. 若$||g_k|| < \epsilon$，则停止迭代；否则确定搜索方向$d_k = -H_k^{-1} \cdot g_k$
> 4. 计算新的迭代点 $x_{k+1} := x_k + d_k$
> 5. 令$k := k + 1$，转至步2
> 
> 当目标函数是二次函数时，由于二次泰勒展开两数与原目标两数不是近似而是完全相同的二次式，海森矩阵退化成一个常数矩阵，从任一初始点出发，利用（1.12）只需一步迭代即可达到$f(x)$的极小点$x^*$，因此牛顿法是一种具有**二次收敛性**的算法。对于非二次函数，若函数的二次性;态较强，或迭代点已进入极小点的邻域，则其收敛速度也是很快的，这是牛顿法的主要优点。

但原始牛顿法由于迭代公式中没有步长因子，而是定步长迭代，对于非二次型目标函数，有时会使函数值上升，即出现$f(x_{k+1}) > f(x_k)$的情况，这表明原始牛顿法**不能保证函数值稳定地下降**，在严重的情况 下甚至可能造成迭代点列$x_k$的发散而导致计算失败.

### §1.2 阻尼牛顿法

为消除1.1中提到的关于原始牛顿法的弊病，人们提出了“**阻尼牛顿法**”，阻尼牛顿法每次迭代的方向仍采用$d_k$，但每次迭代需沿此方向作一维搜索 (line search)，寻求最优的步长因子$\lambda_i$，即

$$
\lambda_k = arg \min\limits_{\lambda \in \mathbb R}f(x_k + \lambda d_k) \tag{1.13}
$$

下面给出一个**阻尼牛顿法**的完整算法描述.

> **算法1.1**（阻尼牛顿法）
> 
> 1. ﻿﻿给定初值$x_0$和精度阀值$\epsilon$，并令$k := 0$.
> 2. ﻿﻿计算$g_k$和$H_k$.
> 3. ﻿﻿若$||g_k||＜ \epsilon$，则停止选代；否则确定搜索方向$d_k = -H_k^{-1} \cdot g_k$.
> 4. 利用(1.13)得到步长$\lambda_k$，并令$x_{k+1} := x_k + \lambda_kd_k$.
> 5. 令$k := k + 1$，转至步2

> **注1.1** 算法1.2的步3中涉及到$H_k^{-1}$的计算，实际应用中，通常并不直接对$H_k$进行求过，而是将其转化为求解线性代数方程组$H_kd_k = -g_k$，此时可根据系数矩阵$H_k$的性态来选择适合的迭代法，如预条件共轭梯度法(PCG)、代数多重网格法(AMG)等。

> **注1.2** 有些文献资料里将算法1.2步3中的搜索方向写成$d_k = H_k^{-1} \cdot g_k$，这是没问题的，原因是在步4中有一个求步长的过程，将搜索方向写成$d_k = H_k^{-1} \cdot g_k$无非是求得的最佳步长和原来求得的最佳步长相差一个符号罢了。

至此已完成了牛顿法的算法介绍，接下来对其做个小结

牛顿法是梯度(下降) 法的进一步发展，梯度法利用目标函数的一阶偏导数信息、以负梯度方向作为搜索方向，只考虑目标函数在迭代点的局部性质；而牛顿法不仅使用目标函数的一阶偏导数，还进一步利用了目标函数的二阶偏导数，这样就考虑了梯度变化的趋势，因而能更全面地确定合适的搜索方向以加快收敛，它具二阶收敛速速． 但牛顿法主要存在以下两个缺点：

- 对目标函数有较严格的要求．函数必须具有连续的一、二阶偏导数，海森矩阵必须正定．
- 计算相当复杂．除需计算梯度外，还需计算二阶偏导数矩阵和它的逆矩阵．计算量、存储量均很大，且均以维数N的平方比增加，当N很大时这个问题更加突出。

## §2 拟牛顿法

如上节所说，牛顿法虽然收敛速度快，但是计算过程中需要计算目标函数的二阶偏导数，计算复杂度较大．而且有时目标函数的海森矩阵无法保持正定，从而使得牛顿法失效．为了克服这两个问题，人们提出了**拟牛顿法**。这个方法的基本思想是：**不用二阶偏导数而构造出可以近似海森矩阵(或海森矩阵的逆）的正定对称阵，在“拟牛顿”的条件下优化目标函数**。不同的构造方法就产生了不同的拟牛顿法．

也有人把“拟牛顿法” 翻译成 “**准牛顿法**”，其实都是表示“**类似于**牛顿法”的意思啦，因为只是对算法中用来计算搜索方向的海森矩阵(或海森矩阵的逆) 作了近似计算罢了．至此，牛顿法和拟牛顿法之间的关系已经跟大家澄清啦.

在介绍具体的拟牛顿法之前，我们先推导一个**拟牛顿条件**，或者叫做**拟牛顿方程**，还有的叫做**割线条件** (Secant condition)．因为对海森矩阵(或海森矩阵的逆）做近似总不能随便近似吧，我们也需要理论指导，而拟牛顿条件则是用来提供理论指导的，它指出了用来近似的矩阵应该满足的条件

为明确起见，下文中用$B$表示对海森矩阵$H$本身的近似，而用 $D$ 表示对海森矩阵的逆$H^{-1}$的近似，即$B \approx H, D \approx H^{-1}$

### §2.1 拟牛顿条件

设经过$k + 1$次选代后得到$x_{k+1}$，此时将目标函数$f(x)$在$x_{k+1}$附近作泰勒展开，取二阶近似，得到

$$
f(x) \approx f(x_{k+1}) + \nabla f(x_{k+1}) \cdot (x-x_{k+1}) + \frac{1}{2} \cdot (x-x_{k+1})^T \cdot \nabla^2f(x_{k+1}) \cdot (x-x_{k+1}) \tag{2.14}
$$

在(2.14)两边同时作用一个梯度算子$\nabla$，可得

$$
\nabla f(x) \approx \nabla f(x_{k+1}) + H_{k+1} \cdot (x - x_{k+1}) \tag{2.15}
$$

在(2.15)中取$x = x_k$并整理，可得

$$
g_{k+1} - g_k \approx H_{k+1} \cdot (x_{k+1} - x_k) \tag{2.16}
$$

若引入记号

$$
s_k = x_{k+1} - x_k, y_k = g_{k+1} - g_k \tag{2.17}
$$

则 (2.16）可紧凑地写成

$$
y_k \approx H_{k+1} \cdot s_k \tag{2.18}
$$

或者

$$
s_k \approx H_{k+1} \cdot y_k \tag{2.19}
$$

这就是所谓的**拟牛顿条件**，它对迭代过程中的海森矩阵$H_{k+1}$作约束，因此，对$H_{k+1}$做近似的$B_{k+1}$，以及对$H_{k+1}^{-1}$做近似的$D_{k+1}$可以将

$$
y_k = B_{k+1} \cdot s_k \tag{2.20}
$$

或者

$$
s_k = D_{k+1} \cdot y_k \tag{2.21}
$$

作为指导

接下来，我们依次介绍几种常见的拟牛顿法．从这里我们可以看出：要想迭代出$x_{k+1}$，就只需要计算$D_{k+1}$即可。**DFP**算法是对$D_{k+1}$的一个近似计算的算法。**BFGS**算法是直接近似计算海森矩阵，用$B_{k+1}$表示。

### §2.2 DFP 算法

DFP算法是以 Wiliain C. Davidon、Roger Fletcher、Michael J. D. Powell三个人的名字的首字母命名的，它由 Davidon 于 1959年首先提出，后经 Fletcher 和 Powell 加以发展和完善，是最早的拟牛顿法，该算法的核心是：通过达代的方法，对$D_{k+1}^{-1}$做近似。迭代格式为

$$
D_{k+1} = D_k + \Delta D_k, k = 0,1,2,\dots \tag{2.22}
$$

其中的$D_0$通常取为单位矩阵$I$. 因此，关键是每一步的校正矩阵$\Delta D_k$如何构造．

注意，迭代格式(2.22）将嵌套在算法 1.2中，因此，我们猜想$\Delta D_k$可能与$s_k, y_k$和$D_k$发生关联． 这里，我们采用“待定法”，即首先将$\Delta D_k$待定成某种形式，然后结合拟牛顿条件(2.21) 来进行推导.

那将$D_k$待定成什么形式呢？这个说起来比较 tricky, 我们将其待定为

$$
\Delta D_k = \alpha uu^T + \beta vv^T \tag{2.23}
$$

其中，$\alpha, \beta$为待定系数，$u, v \in \mathbb R$为待定向量.从形式上看，这种待定公式至少保证了矩阵$\Delta D_k$的**对称性**性（因为$uu^T$和$vv^T$均为对称矩阵）.

将(2.23) 代入 （2.22），并结合指导条件 (2.21)，可得

$$
s_k \approx D_ky_k + \alpha uu^Ty_k + \beta vv^Ty_k \tag{2.24}
$$

从(2.24)似乎也看不出什么啊！别急，我们将其改写一下

$$
s_k = 
\begin{align}
& D_ky_k + u(\color{blue}{\alpha u^Ty_k}) + \color{black}{v}(\color{blue}{\beta v^Ty_k}) \\
& D_ky_k + (\color{blue}{\alpha u^Ty_k})\color{black}{u} + (\color{blue}{\beta v^Ty_k})\color{black}{v} \\
\end{align}

\tag{2.25}
$$

看到了吧？括号中的$\alpha u^Ty_k$和$\beta v^Ty_k$是两个数，既然是数，我们不妨作如下简单赋值

$$
\alpha u^Ty_k = 1, \beta v^Ty_k = -1 \tag{2.26}
$$

即

$$
\alpha = \frac{1}{u^Ty_k}, \beta = -\frac{1}{v^Ty_k} \tag{2.27}
$$

其中向量$u,v$仍有待确定．

那么，$u, v$如何确定呢？将（2.26）代入(2.25)，得到

$$
u - v = s_k - D_ky_k \tag{2.28}
$$

要上式成立，不妨直接取

$$
u = s_k, v = D_ky_k \tag{2.29}
$$

再将（2.29）代入（2.27），便得

$$
\alpha = \frac{1}{s_k^Ty_k}, \beta = -\frac{1}{(D_ky_k)^Ty_k} = -\frac{1}{y_k^TD_ky_k} \tag{2.30}
$$

其中第二个等式用到了 $D_k$ 的对称性

至此，我已经将校正矩阵$\Delta D_k$构造出来了，将（2.29）和（2.30） 代入（2.23），便得

$$
\Delta D_k = \frac{s_ks_k^T}{s_k^Ty_k} - \frac{D_ky_ky_k^TD_k}{y_k^TD_ky_k} \tag{2.31}
$$

综上，我们给出DFP算法的一个完整算法描述.

> **算法 2.1**  (DFP 算法）
> 
> 1. 给定初值$x_0$和精度阀值$\epsilon$，并令$D_0 = I, k := 0$.
> 2. 确定搜索方向 $d_k = -D_k \cdot g_k$
> 3. ﻿﻿利用（1.13）得到步长$\lambda_k$, 令$s_k = \lambda_kd_k, x_{k+1} := x_k + s_k$
> 4. ﻿若$||g_{k+1}|| < \epsilon$，则算法结束．
> 5. ﻿﻿计算$y_k = g_{k+1} - g_k$.
> 6. ﻿﻿﻿计算$D_{k+1} = D_k + \frac{s_ks_k^T}{s_k^Ty_k} - \frac{D_ky_ky_k^TD_k}{y_k^TD_ky_k}$
> 7. ﻿﻿﻿令$k := k + 1$， 转至步2

### §2.3 BFGS 算法

BFGS 算法是以其发明者Broyden，Fletcher，Goldtarb和Shanno四个人的名字的首字母命名的。与DFP算法相比，BFGS 算法性能更佳．日前它已成为求解无约束非线性优化问题最常用的方法之一．BFGS 算法已有较完善的局部收敛理论，对其全局收敛性的研究也取得了重要成果.

BFGS 算法中核心公式的推导过程和 DFP 完全类似，只是互换了其中$s_k$和$y_k$的位置.为方便自己以后查阅，我打算将上一节的推导过程再重写一遍，已经领会该过程的读者则不妨跳过以下推导，直接看结论．

需要注意的是，BFGS 算法是直接逼近海森矩阵，即$B_k \approx H_k$． 仍采用迭代方法，设迭代格式为

$$
B_{k+1} = B_k + \Delta B_{k}, k=0,1,2,\dots \tag{2.32}
$$

其中的$B_0$也常取为单位矩阵$I$．因此，关键是每一步的校正矩阵$\Delta B_{k}$如何构造．同样，将其待定为

$$
\Delta B_{k} = \alpha uu^T + \beta vv^T \tag{2.33}
$$

将(2.33)代入 (2.32），并结合指导条件 (2.20)，可得

$$
s_k = B_ks_k + (\color{blue}{\alpha u^Ts_k})\color{black}{u} + (\color{blue}{\beta v^Ts_k})\color{black}{v} \tag{2.25}
$$

通过令$\alpha u^Ts_k = 1, \beta v^Ts_k = -1 \tag{2.26}$,以及

$$
u = y_k, v = B_ks_k \tag{2.35}
$$

可算得

$$
\alpha = \frac{1}{y_k^Ts_k}, \beta = -\frac{1}{s_k^TB_ks_k} \tag{2.36}
$$

综上，便得到了如下校正矩阵$\Delta B_k$的公式

$$
\Delta B_k = \frac{y_ky_k^T}{y_k^Ts_k} - \frac{B_ks_ks_k^TB_k}{s_k^TB_ks_k} \tag{2.37}
$$

好了，现在把矩阵$\Delta B_k$和$\Delta D_k$拿出来对比一下，是不是除了将$D$换成$B$外，其它只是将$s_k$和$y_k$互调了一下位置呢？

最后，我们给出BFGS算法的一个完整算法描述.

> **算法 2.2**  (BFGS 算法(I)）
> 
> 1. 给定初值$x_0$和精度阀值$\epsilon$，并令$B_0 = I, k := 0$.
> 2. 确定搜索方向 $d_k = -B_k^{-1} \cdot g_k$
> 3. ﻿﻿利用（1.13）得到步长$\lambda_k$, 令$s_k = \lambda_kd_k, x_{k+1} := x_k + s_k$
> 4. ﻿若$||g_{k+1}|| < \epsilon$，则算法结束．
> 5. ﻿﻿计算$y_k = g_{k+1} - g_k$.
> 6. ﻿﻿﻿计算$B_{k+1} = B_k + \frac{y_ky_k^T}{y_k^Ts_k} - \frac{B_ks_ks_k^TB_k}{s_k^TB_ks_k}$
> 7. ﻿﻿﻿令$k := k + 1$， 转至步2

算法 2.2中的步 2通常是通过求解线性代数方程组$B_kd_k = -g_k$来进行． 然而，更一般的做法是，通过对步6中的递推关系应用 Sherman-Morrison 公式，直接给出$B_{k+1}^{-1}$与$B_k^{-1}$之间的关系式

$$
B_{k+1}^{-1} = (I - \frac{s_ky_k^T}{y_k^Ts_k})B_k^{-1}(I - \frac{y_ks_k^T}{y_k^Ts_k}) + \frac{s_ks_k^T}{y_k^Ts_k} \tag{2.38}
$$

或者迸一步展开写成

$$
B_{k+1}^{-1} = B_k^{-1} + \color{blue}{(\frac{1}{s_k^Ty_k} + \frac{y_k^TB_k^{-1}y_k}{(s_k^Ty_k)^2})}\color{black}{s_ks_k^T} - \color{blue}{\frac{1}{s_k^Ty_k}}\color{black}{(B_k^{-1}y_ks_k^T + s_ky_k^TB_k^{-1})} \tag{2.39}
$$

其中的蓝色部分表示实数.

> 注 2.1 关于**Sherman-Morrison**公式
> 
> 设$A \in \mathbb R^n$为非奇异方阵，$u,v \in \mathbb R^n$，若$1 + v^TA^{-1}u \ne 0$，则有
> 
> $$
> (A + uv^T)^{-1} = A^{-1} - \frac{A^{-1}uv^TA^{-1}}{1 + v^TA^{-1}u} \tag{2.40}
> $$

利用（2.38)，我们很容易将算法 2.2 改写成算法2.3. 注意，为了避免出现矩阵求逆符号，我们统一将$B_i^{-1}$换用$D_i$(这样做仅仅只是符号上看起来舒服起见)。这样，整个算法中不再需要求解线性代数方程组，由矩阵 - 向量运算就可以完成了。

> **算法2.3** （BFGS 算法II）
>
> 1. ﻿﻿给定初值$x_0$和精度阀值$\epsilon$，并令$D_0 = I, k := 0$.
> 2. ﻿﻿﻿确定捜索方向$d_k = -D_k \cdot g_k$.
> 3. ﻿﻿﻿利用（1.13）得到步长$\lambda_k$，令$s_k = \lambda_kd_k, x_{k+1} := x_k + s_k$
> 4. ﻿﻿﻿若$||g_{k+1}||< \epsilon$， 则算法结束
> 5. ﻿﻿﻿计算$y_k = g_{k+1} - g_{k}$
> 6. 计算$D_{k+1} = (I - \frac{s_ky_k^T}{y_k^Ts_k})D_k(I - \frac{y_ks_k^T}{y_k^Ts_k}) + \frac{s_ks_k^T}{y_k^Ts_k}$
> 7. 令$k := k + 1$，转至步2

至此，关于 DFP 算法和 BFGS 算法的介绍就完成了，回过头来，我们对比一下算法 2.1和算法 2.3，容易发现，这两个算法的唯一不同仅在于 $D_{k+1}$的迭代公式不同罢了．

最后，再补充谈谈一维搜索(line search）的问题，在之前几个算法描述中，为简单起见，均采用（1.13） 来计算步长$\lambda_k$，其实这是一种**精确搜索**．实际应用中，还有像Wolfe 型搜索、Armijo 搜素以及满足 Goldstein 条件的**非精确搜索**．这里我们以 Wolte 搜索为例，简单做个介绍.

设$\widetilde{\beta} \in (0, \frac{1}{2}), \beta \in (\widetilde{\beta}, 1)$，所谓的 Wolfe 搜索是指$\lambda_k$满足如下**Wolfe条件**

$$
\begin{cases}

f(x_k + \lambda_kd_k) \le f(x_k) + \widetilde{\beta}\lambda_kd_k^Tg_k; \\
d_k^Tg(x_k + \lambda_kd_k) \ge \beta d_k^Tg_k

\end{cases}

\tag{2.41}
$$

带非精确搜索的拟牛顿法的研究是从 1976 年Powell 的工作开始的，他证明了带 Wolfe 搜索的 BFGS 算法的全局收敛性和超线性收敛性．

### §2.4 L-BFGS 算法

在 BFGS 算法中，需要用到一个$N \times N$ 的矩阵 $D_k$，当 $N$ 很大时，存储这个矩阵将变得很耗计算机资源．例如(2)，考虑$N$为10万的情形，且用 double 型(8 字节)来存储$D_k$，需要多大的内存呢？我们来计算一下

$$
\frac{N 阶矩阵的字节数}{1 GB 的字节数} = \frac{10^5 \times 10^5 \times 8}{2^{10} \times 2^{10} \times 2^{10}} = 74.5(GB) \tag{2.42}
$$

74.5 GB, 很惊人是吧，这对于一般的服务器是很难承受的．当然，考虑到矩阵$D_k$的对称性，内存还可以降一半，但是，在机器学习问题中，像10万这样的规模还只能算是中小规模.那么，是否可以通过对 BFGS 算法进行改造，从而减少其迭代过程中所需的内存开销呢？

答案是肯定的，L-BFGS (Limited-memory BFGs 或 Limited-storage BFGS) 算法就是这样一种算法．它对 BFGS 算法进行了近似，其基本思想是：不再存储完整的矩阵 $D_k$，而是存储计算过程中的向量序列$\{s_i\}, \{y_i\}$，需要矩阵$D_k$时，利用向量序列$\{s_i\}, \{y_i\}$的计算来代替．而且，向量序列$\{s_i\}, \{y_i\}$也不是所有的都存，而是固定存最新的m个（参数m可由用户根据自己机器的内存自行指定)．每次计算$D_k$时，只利用最新的m个$\{s_i\}$和m个$\{y_i\}$．显然，这样一来，我们将存储由原来的 $O(N^2)$降到了 $O(mN)$

接下来，讨论 L-BFGS 算法的具体实现过程．我们的出发点是算法 2.3步6中的迭代式

$$
D_{k+1} = (I - \frac{s_ky_k^T}{y_k^Ts_k})D_k(I - \frac{y_ks_k^T}{y_k^Ts_k}) + \frac{s_ks_k^T}{y_k^Ts_k}
$$

若记$\rho_k = \frac{1}{y_k^Ts_k}, V_k = I - \rho_ky_ks_k^T$，则上式可写成

$$
D_{k+1} = V_k^TD_kV_k + \rho_ks_ks_k^T \tag{2.43}
$$

如果给定初始矩阵$D_0$（通常为正定的对角矩阵，如 $D_0 = 1$），则利用(2.43)， 依次可得

$$
\begin{align}
D_1 & = V_0^TD_0V_0 + \rho_0s_0s_0^T; \\
D_2 & = V_1^TD_1V_1 + \rho_1s_1s_1^T \\
& = V_1^T(V_0^TD_0V_0 + \rho_0s_0s_0^T)V_1 + \rho_1s_1s_1^T \\
& = V_1^TV_0^TD_0V_0V_1 + V_1^T\rho_0s_0s_0^TV_1 + \rho_1s_1s_1^T;\\
D_3 & = V_2^TD_2V_2 + \rho_2s_2s_2^T \\
& = V_2^T(V_1^TV_0^TD_0V_0V_1 + V_1^T\rho_0s_0s_0^TV_1 + \rho_1s_1s_1^T)V_2 + \rho_2s_2s_2^T \\
& = V_2^TV_1^TV_0^TD_0V_0V_1V_2 + V_2^TV_1^T\rho_0s_0s_0^TV_1V_2 + V_2^T\rho_1s_1s_1^TV_2 + \rho_2s_2s_2^T; \\
...
\end{align}
$$

一般地，我们有

$$
\begin{align}
D_{k+1} & = (V_k^TV_{k-1}^T\dots V_1^TV_0^T)D_0(V_0V_1 \dots V_{k-1}V_k) \\
& + (V_k^TV_{k-1}^T\dots V_2^TV_1^T)(\rho_0s_0s_0^T)(V_1V_2 \dots V_{k-1}V_k) \\
& + (V_k^TV_{k-1}^T\dots V_3^TV_2^T)(\rho_1s_1s_1^T)(V_2V_3 \dots V_{k-1}V_k) \\
& + \dots \\
& + (V_k^TV_{k-1}^T)(\rho_{k-2}s_{k-2}s_{k-2}^T)(V_{k-1}V_k) \\
& + V_k(\rho_{k-1}s_{k-1}s_{k-1}^T)V_k \\
& + \rho_ks_ks_k^T
\end{align}

\tag{2.44}
$$

由上式可见，计算$D_{k+1}$需用到$\{s_i,y_i\}_{i=0}^k$，因此，若从$s_0,y_0$开始连续地存储m 组的话，只能存储到$s_{m-1}, y_{m-1}$，亦即，只能依次计算$D_1,D_2,\dots$，直到$D_m$．那么，$D_{m+1},D_{m+2}$该如何计算呢？

自然地，如果一定要丟掉一些向量，那么肯定优先考志那些最早生成的向量．具体来说，计算$D_{m+1}$时，我们保存$\{s_i,y_i\}_{i=1}^m$，丢掉了$\{x_0,y_0\}$;计算$D_{m+2}$时，我们保存$\{s_i,y_i\}_{i=2}^{m+1}$，丟掉了$\{x_i, y_i\}_{i=0}^1;\dots$

但是舍弃掉一些向量后，就只能**近似计算**了，当$k+1>m$时，仿照(2.44)，可以构造近似计算公式

$$
\begin{align}
D_{k+1} & = (V_k^TV_{k-1}^T\dots V_{k-m+2}^TV_{k-m+1}^T)D_0(V_{k-m+1}V_{k-m+2} \dots V_{k-1}V_k) \\
& + (V_k^TV_{k-1}^T\dots V_{k-m+3}^TV_{k-m+2}^T)(\rho_0s_0s_0^T)(V_{k-m+2}V_{k-m+3} \dots V_{k-1}V_k) \\
& + (V_k^TV_{k-1}^T\dots V_{k-m+4}^TV_{k-m+3}^T)(\rho_1s_1s_1^T)(V_{k-m+3}V_{k-m+4} \dots V_{k-1}V_k) \\
& + \dots \\
& + (V_k^TV_{k-1}^T)(\rho_{k-2}s_{k-2}s_{k-2}^T)(V_{k-1}V_k) \\
& + V_k(\rho_{k-1}s_{k-1}s_{k-1}^T)V_k \\
& + \rho_ks_ks_k^T
\end{align}

\tag{2.45}
$$

在文献181中，(2.44） 和（2.45） 被称为 special BFGS matrices. 若引人$\hat{m} = \min\{k,m-1\}$，则还可以将(2.44)和(2.45)合并地写成

$$
\begin{align}
D_{k+1} & = (V_k^TV_{k-1}^T\dots V_{k-\hat{m}+1}^TV_{k-\hat{m}}^T)D_0(V_{k-\hat{m}}V_{k-\hat{m}+1} \dots V_{k-1}V_k) \\
& + (V_k^TV_{k-1}^T\dots V_{k-\hat{m}+2}^TV_{k-\hat{m}+1}^T)(\rho_0s_0s_0^T)(V_{k-\hat{m}+1}V_{k-\hat{m}+2} \dots V_{k-1}V_k) \\
& + (V_k^TV_{k-1}^T\dots V_{k-\hat{m}+3}^TV_{k-\hat{m}+2}^T)(\rho_1s_1s_1^T)(V_{k-\hat{m}+2}V_{k-\hat{m}+3} \dots V_{k-1}V_k) \\
& + \dots \\
& + (V_k^TV_{k-1}^T)(\rho_{k-2}s_{k-2}s_{k-2}^T)(V_{k-1}V_k) \\
& + V_k(\rho_{k-1}s_{k-1}s_{k-1}^T)V_k \\
& + \rho_ks_ks_k^T
\end{align}

\tag{2.46}
$$

看到这里，读者千万不要被(2.46）的冗长复杂的形式所吓到．事实上，由 BFGS 算法流程易知，$D_k$ 的作用仅用来计算 $D_kg_k$ 获取搜索方向，因此，若能利用表达式 (2.46) 设计出一种计算 $D_kg_k$ 的快速算法，则大功告成. 文 (8给出了这样一个算法，具体见算法 2.4.

> **算法 2.4**（$D_k \cdot g_k$ 的快速算法）
> 
> Step 1 初始化.
> 
> $$
> \delta = 
> \begin{cases}
> 0, & 若 k \le m \\
> k - m, & 若 k > m
> \end{cases}
> ;
> 
> L = 
> \begin{cases}
> 0, & 若 k \le m \\
> k, & 若 k > m
> \end{cases}
> ;
> 
> q_L = g_k.
> $$
> 
> Step2后向循环
> 
> ```
> FOR i = L - 1, L - 2, ..., 1, 0 DO
> {
> 	j = i + 𝛿;
> 	⍺_i = ⍴_js_j^Tq_{i+1}; // ⍺_i需要存下来，前向循环要用！
> 	q_i = q_{i+1} - ⍺_iy_j.
> }
> ```
> 
> Step 3 前向循环
> 
> ```
> r_0 = D_0q_0;
> FOR i = 0,1,...,L-2,L-1 DO
> {
> 	j = i + 𝛿;
> 	β_j = ⍴_jy_j^Tr_i;
> 	r_{i+1} = r_i + (⍺_i - β_i)s_j;
> }
> ```

最后算出的$r_L$即力 $H_k \cdot g_k$ 的值．

































