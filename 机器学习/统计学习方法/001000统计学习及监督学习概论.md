# 统计学习及监督学习概论

## 1 统计学习

**统计学习**是关于计算机基于数据构建概率统计模型并运用 模型对数据进行预测与分析的一门学科。

### 统计学习特点

1. 统计学习以计算机及网络为平台，是建立在计算机 及网络上的; 
2. 统计 学习以数据为研究对象，是数据驱动的学科 ; 
3. 统计学习的目 的是对数据进行预测与分析; 
4. 统计学习以方法为中心，统计学习方法构建模型井 应用模型进行预测与分析; 
5. 统计学习是概率论、统计学、信息论、计算理论、最优 化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独自的理论体系 与方法论。

### 统计学习的对象

统计学习研究的对象是数据。

### 统计学习的目的

统计学习用于对数据的预测与分析，特别是对未知新数据的预测与分析。

### 统计学习的方法

- 模型的假设空间
- 模型选择的准则
- 模型学习的算法

**实现统计学习方法的步骤如下**

1. 得到 一 个有限的训练、数据集合:
2. 确定包含所有可能的模型的假设空间，即学习模型的集合;
3. 确定模型选择的准则，即学习的策略;
4. 实现求解最优模型的算法，即学习的算法:
5. 通过学习方法选择最优模型:
6. 利用学习的最优模型对新数据进行预测或分析。

## 2 分类

### 基本分类

- **监督学习**
- **无监督学习**
- **强化学习**
- 半监督学习: 
- 主动学习:

#### 监督学习

![](../assets/imgs/ai/jianduxuexi.png)

**输入空间**和**输出空间**

将输入与输出所有可能取值的集合分别称为**输入空间**与**输出空间**。输入与输出空间可以是有限元素的集合，也可以是整个欧氏空间。

**实例**

每个具体的输入是一个实例，通常由特征向量表示。

**特征空间**

所有特征向量存在的空间称为**特征空间**。特征空间的每一维对 应于一个**特征**。

**数学表示**

输入变量写作$X$， 输出变量写作$Y$。输入变量的取值写作$x$，输出变量的取值写作$y$。

输入实例$x$的特征向量记作

$$
x = (x^{(1)},x^{(2)},\dots,x^{(i)},\dots,x^{(n)})^T
$$

$x_i$表示多个输入变量中的第$i$个变量

$$
x_i = (x_i^{(1)},x_i^{(2)},\dots,x_i^{(i)},\dots,x_i^{(n)})^T
$$

测试数据也由输入与输出对组成。输入与输出对又称为**样本**或样本点。

训练集表示为

$$
T = \{(x_1,y_1),(x_2,y_2),\dots,(x_n,y_n)\}
$$

**联合概率分布**

监督学习假设输入与输出的随机变量$X$和$Y$遵循联合概率分布$P(X，Y)$。$P(X，Y)$表示分布函数，或分布密度函数 。

**假设空间**

模型属于由*输入空间*到*输出空间*的映射的集合，这个集合就是**假设空间**。用$\mathcal F$表示

#### 无监督学习

**无监督学习**是指从无标注数据中学习预测模型的机器 学习问题。

![](../assets/imgs/ai/wujianduxuexi.png)

模型可以实现对数据的:
- 聚类
- 降维
- 概率估计

#### 强化学习

**强化学习**是指智能系统在与环境的连续互动中学习 最优行为策略的机器学习问题。

![](../assets/imgs/ai/qianghuaxuexi.png)

#### 半监督学习&主动学习

**半监督学习**是指利用 标注数据和未标注数据学习预 测模型的机器学习问题。

**主动学习**是指机器不断主动给出实例让教师进行标注，然后利 用标注数据学习预测模型的机器学习问题。

### 按模型分类

#### 1 概率模型与非概率模型

- 概率模型
- 非概率模型
- 确定性模型

**概率模型**

- 决策树
- 朴素贝叶斯
- 隐马尔可夫模型
- 条件随机场
- 概率潜在语义分析
- 潜在狄利克雷分配
- 高斯温合模型

**非概率模型**

- 感知机
- 支持向量机
- k近邻
- AdaBoost
- k均值
- 潜在语义分析
- 神经网络
- 逻辑斯谛回归（既可看作是概率模型，又可看作是非概率模型）

#### 2. 线性模型与非线性模型

- 性模型
- 非线性模型

**线性模型**

- 感知机
- 线性支持向量机
- k 近邻
- k 均值
- 潜在语义分析

**非线性模型**

- 核函数支持向量机
- AdaBoost
- 神经网络

#### 3. 参数化模型与非参数化模型

- 参数化模型
- 非参数化模型

**参数化模型**

- 感知机
- 朴素贝叶斯
- 逻辑斯谛回归
- k 均值
- 高斯混合模型

**非参数化模型**

- 决策树
- 支持向量机
- AdaBoost 
- k近邻
- 潜在语义分析
- 概率潜在语义分析
- 潜在狄利克雷分配

### 按算法分类

- 在线学习 (online learning) : 是指每次接受一个样本，进行预测，之后学习模型，并不断重复该操作的机器学习。
- 批量学习 (batch learning) : 一 次接受所有数据，学习模型，之后进行预 测。 

### 按技巧分类

- 贝叶斯学习
- 核万法

## 3 三要素

$$
方法=模型+策略+算法
$$

- 模型
- 策略
- 算法

### 模型

**模型**就是所要学习的条件概率分布或决策函数。

假设空间定义为决策函数的集合：

$$
\mathcal F = \{f|Y=f(X)\}
$$

- X：定义在输入空间$\mathcal X$上的变量
- Y：定义在输入空间$\mathcal Y$上的变量

这时$\mathcal F$通常是由一个参数向量决定的函数族 :

$$
\mathcal F = \{f|Y=f_{\theta}(X),\theta \in \bf R^n\}
$$

假设空间定义为条件概率的集合：

$$
\mathcal F = \{P|P(Y|X)\}
$$

这时$\mathcal F$通常是由一 个参数向量决定的条件概率分布族:

$$
\mathcal F = \{P|P_{\theta}(Y|X),\theta \in \bf R^n\}
$$

### 策略

#### 损失函数

定义：**损失函数**是$f(X)$和$Y$的非负实值函数，记作$L(Y,f(X))$。

作用：损失函数度量模型一次预测的好坏

常用的损失函数有：

**0-1 损失函数**

$$
L(Y,f(X)) =

\begin{cases}
1, Y \ne f(X) \\
0, Y = f(X)
\end{cases}
$$

**平方损失函数**

$$
L(Y,f(X)) = (Y - f(X))^2
$$

**绝对损失函数**

$$
L(Y,f(X)) = |Y - f(X)|
$$

**对数损失函数或对数似然损失函数**

$$
L(Y,f(X)) = -logP(P|X)
$$

#### 风险函数

##### 风险函数/期望损失

损失函数的期望是

$$
R_{exp}(f) = E_P[L(Y,f(X))] = \int_{\mathcal X \times \mathcal Y}L(y,f(x))P(x,y)dxdy
$$

这是理论上模型 $f(X)$ 关于联合分布 $P(X，Y)$ 的平均意义下的损失，称为**风险函数** (risk function) 或**期望损失** (expected loss) 。

##### 经验风险/经验损失

给定一个训练数据集

$$
T = \{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}
$$

模型 $f(X)$ 关于训练数据集的平均损失称为经验风险 (empirical risk) 或经验损 失 (empirical loss) ，记作 $R_{emp}$:

$$
R_{emp}(f) = \frac{1}{N}\sum_{i=1}^NL(y_i,f(i))
$$

学习的目标就是选择期望风险最小的模型。

##### 经验风险最小化与结构风险最小化

**经验风险最小化** (ERM) 的策略认为，经验风险最小的模型是最优的模型。

$$
\min_{f \in \mathcal F} \frac{1}{N}\sum_{i=1}^NL(y_i,f(i))
$$

**结构风险最小化** (SRM) 是为了防止*过拟合*而提出来的策略。结构风险最小化等价于**正则化**。结构风险在经验风险上加 上表示模型复杂度的**正则化项**或**罚项**。

$$
R_{srm}(f) = \frac{1}{N}\sum_{i=1}^NL(y_i,f(i)) + \lambda J(f)
$$

$\lambda \ge 0$。模型$f$越复杂，复杂度$J(f)$就越大:反之，模型$f$越简单，复杂度$J(f)$就越小。

### 算法

算法是指学习模型的具体计算方法。统计学习的算法成为求解最优化问题的算法。

## 4 模型评估与模型选择

#### 训练误差与测试误差

假设学习到的模型是$Y = \hat{f}(X)$. 训练误差是模型$Y = \hat{f}(X)$关于训练数据集的平均损失:

$$
R_{emp}(\hat{f}) = \frac{1}{N}\sum_{i=1}^NL(y_i,\hat{f}(i))
$$

其中 N 是训练样本容量。

测试误差是模型$Y = \hat{f}(X)$关于测试数据集的平均损失:

$$
e_{test} = \frac{1}{N^{'}}\sum_{i=1}^{N^{'}}L(y_i,\hat{f}(i))
$$

其中$N^{'}$是测试样本容量。

#### 过拟合与模型选择

如果一味追求提高对训练数据的预测能力，所选模型的复杂度则往往会比真模型更高。这种现象称为**过拟合**。

**过拟合**是指模型对己知数据预测很好，对未知数据预测得很差的现象。

## 5 正则化与交叉验证

### 正则化

正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项

$$
\min_{f \in \mathcal F} \frac{1}{N}\sum_{i=1}^NL(y_i,f(i)) + \lambda J(f)
$$

### 交叉验证

随机地将数据集切分成三部分，分别为

- 训练集：用来训练模型
- 验证集：用于模型的选择
- 测试集：测试集用于对学习方法的评估

- 简单交叉验证
- S折交叉验证
- 留一交叉验证

## 6 泛化能力

学习方法的**泛化能力**是指由该方法学习到的模型对未知数据的预测能力

## 7 生成模型与判别模型

生成方法由数据学习联合概率分布$P(X,Y)$， 然后求出条件概率分布 $P(Y|X)$作 为预测的模型，即生成模型:

$$
P(Y|X) = \frac{P(X,Y)}{P(X)}
$$

判别方法由数据直接学习决策函数 $f(X)$ 或者条件概率分布 $P(Y|X)$ 作为预测的 模型，即判别模型。

## 8 监督学习应用

- 分类问题
- 标注问题
- 回归问题











