# 第3章 k近邻法

k近邻法(kNN)是一种**基本分类**与回归方法。k 近邻法的输入为实例的特征向量，对应于特征空间的点: 输出为实例的类别，**可以取多类**。

## 3.1 k 近邻算法

直观理解：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 k 个实例，这 k 个实例的多数属于某个类，就把该输入实例分为这个类。

> 我的理解
> 
> 操场上有两拨人，一拨男生，一拨女生。现在随机选一个人，不知道他的性别，但是知道他最近的人是男生，那么也认为这个人是男生

### 算法 3.1 (k 近邻法)

输入 :训练数据集

$$
T = \{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\}
$$

其中，$x_i \in \mathcal X \subseteq \bf{R}^n$为实例的特征向量，$y_i = \mathcal Y = \{c_1,c_2,\dots,c_K\}$为实例的类别，$i=1,2,\dots,N$; 实例特征向量$x$:

输出:实例$x$所属的类$y$。

- （1）根据给定的距离度量，在训练集$T$中找出与$x$最邻近的$k$个点，涵盖这$k$个点的$x$的邻域记作 $N_k(x)$；
- （2）在$N_k(x)$中根如分类决策规则(如**多数表决**)决定$x$的类别$y$:

$$
y = arg max_{c_j} \sum_{x_i \in N_k(x)}I(y_i = c_j), i=1,2,\dots,N;j=1,2,\dots,K
$$

- $I$为**指示函数**，即当$y_i = c_j$时$I$为1，否则$I$为0

$k=1$的情形，称为**最近邻算法** 。

## 3.2 k近邻模型

模型三个要素

- 距离度量
- k 值的选择
- 分类决策规则决定

### 3.2.1 模型

### 3.2.2 距离度量

设特征空间$\mathcal X$是$n$维实数向量空间$\bf{R}^n$，$x_i,x_j \in \mathcal X, x_i = (x_i^{(1)},x_i^{(2)}, \dots, x_i^{(n)})^T, x_j = (x_j^{(1)},x_j^{(2)}, \dots, x_j^{(n)})^T$，$x_i,x_j$的$L_p$距离定义为

$$
L_p(x_i,x_j) = (\sum_{l=1}^n|x_i^{(l)} - x_j^{(l)}|^p)^{\frac{1}{p}}
$$

这里$p \ge 1$。

当$p=2$时，称为**欧氏距离** ，即

$$
L_2(x_i,x_j) = (\sum_{l=1}^n|x_i^{(l)} - x_j^{(l)}|^2)^{\frac{1}{2}}
$$

当$p=1$时，称为**曼哈顿距离** ，即

$$
L_1(x_i,x_j) = \sum_{l=1}^n|x_i^{(l)} - x_j^{(l)}|
$$

当$p=\infty$时，它是各个坐标距离的最大值，即

$$
L_\infty(x_i,x_j) = \max_{l=1}|x_i^{(l)} - x_j^{(l)}|
$$

![](../assets/imgs/ai/lp.png)

### 3.2.3 k值的选择

$k$值的减小就意味着整体模型变得复杂，容易发生过拟合。

$k$值一般取一个比较小的数值。通常采用交叉验证法来选取最优的$k$值。

### 3.2.4 分类决策规则

**多数表决**

如果分类的损失函数为 0-1 损 失函数，分类函数为

$$
f: \bf{R}^n \to \{c_1,c_2,\dots,c_K\}
$$

那么误分类的概率是

$$
P(Y \ne f(X)) = 1 - P(Y = f(X))
$$

对给定的实例$x \in \mathcal X$， 其最近邻的$k$个训练实例点构成集合 $N_k(X)$。 如果涵盖$N_k(X)$的区域的类别是$c_j$那么误分类率是

$$
\frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i \ne c_j) = 1 - \frac{1}{k} \sum_{x_i \in N_k(x)} I(y_i = c_j)
$$

要使误分类率最小即经验风险最小，就要使$\sum_{x_i \in N_k(x)} I(y_i = c_j)$最大

## 3.3 k近邻法的实现:kd树

### 3.3.1 何造kd树

> **中位数**：一组数据按大小顺序排列起来，处在中间位置的一个数或最中间两个数的平均值。
> 
> 实操过程：有$n$个数，就取$n / 2$取整，因为平均数可能没有对应的数据点

#### 算法 3.2 (构造平衡 kd 树)

输入: $k$维空间数据集$T = \{x_1,x_2,\dots,x_N\}$，其中$x_i = x_i^{(2)}, \dots, x_i^{(n)})^T$，$i=1,2,\dots,N$；

输出: kd树。

- （1）开始：构造根结点，根结点对应于包含$T$的$k$维空间的超矩形区域。

选择$x^{(1)}$为坐标轴，以$T$中所有实例的$x^{(1)}$坐标的中位数为切分点，将根结点对应的超矩形区域切分为两个子区域。切分由通过切分点并与坐标轴$x^{(1)}$垂直的超平面实现。

由根结点生成深度为$1$的左、右子结点：左子结点对应坐标$x^{(1)}$小于切分点的子区域，右子结点对应于坐标$x^{(1)}$大于切分点的子区域。

将落在切分超平面上的实例点保存在根结点。

- （2）重复:对深度为$j$的结点，选择$x^{(l)}$为切分的坐标轴，$l = j(mod k) + 1$，以该结点的区域中所有实例的$x^{(l)}$坐标的中位数为切分点，将该结点对应的超矩形区域切分为两个子区域。切分由通过切分点井与坐标轴$x^{(l)}$垂直的超平面实现。

由该结点生成深度为$j+1$的左、右子结点：左子结点对应坐标$x^{(l)}$小于切分点的子区域， 右子结点对应坐标$x^{(l)}$大于切分点的子区域。 

将落在切分超平面上的实例点保存在该结点。  

- （3）直到两个子区域没有实例存在时停止 。 从而形成kd树的区域划分

### 3.3.2 搜索kd树

#### 算法 3.3 (用 kd 树的最近邻搜索)

输入：己构造的kd树，目标点$x$;  

输出 ：$x$的最近邻。

- （1）在kd树中找出包含目标点 z 的叶结点：从根结点出发，递归地向下访问kd树。若目标点$x$当前维的坐标小于切分点的坐标，则移动到左子结点，否则移动到右子结点。直到子结点为叶结点为止。
- （2）以此叶结点为"**当前最近点**"。  
- （3）**递归**地向上回退，在每个结点进行以下操作:
	- （a）如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为"**当前最近点**" 。  
	- （b）当前最近点一定存在于该结点一个子结点对应的区域。 检查该子结点的父结点的另一子结点对应的区域是否有更近的点。具体地，检查另一子结点对应的区域是否与**以目标点为球心、以目标点与"当前最近点"间的距离为半径的超球体相交**。

		**如果相交**，可能在另一个子结点对应的区域内存在距目标点更近的点，移动 到另一个子结点。接着，递归地进行最近邻搜索;

		**如果不相交**，向上回退。

- （4）当回退到根结点时，搜索结束。最后的"当前最近点"即为$x$的最近邻点。










